{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of other results\n",
    "\n",
    "This notebook has been written to run on the TU Ilmenau cluster with my specific setup.\n",
    "However, it can also be used to simply visualize the effect of changing the 'spectra_dim'\n",
    "of the models and to investigate whether adding an 'MLP+AddNorm' block to each message passing layer improves performance.\n",
    "\n",
    "A summary of all results is also stored in the directory `/research/other_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "import optimetal.utils as utils\n",
    "import optimetal.factory as factory\n",
    "from optimetal.data.loader import load_torch_data\n",
    "utils.load_plot_style()\n",
    "\n",
    "def load_tb_scalars(logdir: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load scalar values from tensorboard event files. This is useful\n",
    "    when you want to look at training and validation loss curves.\n",
    "    \"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        logdir,\n",
    "        size_guidance={event_accumulator.SCALARS: 0},\n",
    "    )\n",
    "    ea.Reload()\n",
    "    tags = ea.Tags().get(\"scalars\", [])\n",
    "    tb_log = {}\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        values = [e.value for e in events]\n",
    "        tb_log[tag] = values\n",
    "    return tb_log\n",
    "\n",
    "def load_results(study_path: str, mode: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load all of results from the transformer hyperparameter scaling law study.\n",
    "    Input:\n",
    "        study_path:     Path to the root directory containing subdirectories from model training\n",
    "        mode:           Supported modes are 'spectra_dim' and 'mlp_addnorm'\n",
    "    Output:\n",
    "        scaling_laws:   Nested dict mapping, model type, scaling type, and hyperparameters to dictionaries with \"val_loss\"\n",
    "    \"\"\"\n",
    "\n",
    "    # gather all study directories\n",
    "    study_dirs = [d for d in os.listdir(study_path) if os.path.isdir(os.path.join(study_path, d))]\n",
    "    # initialize nested structure for results\n",
    "    results = {}\n",
    "    # iterate through each study directory and load results\n",
    "    print(f\"Loading scaling law results for {len(study_dirs):d} models\")\n",
    "    for study_dir in study_dirs:\n",
    "        # path setup and checks\n",
    "        study_dir_path = os.path.join(study_path, study_dir)\n",
    "        val_loss_path = os.path.join(study_dir_path, \"val_loss.txt\")\n",
    "        best_model_path = os.path.join(study_dir_path, \"best_model.pt\")\n",
    "        if not os.path.exists(val_loss_path) or not os.path.exists(best_model_path):\n",
    "            print(f\"Skipping {study_dir_path:s}, probably still running\")\n",
    "            continue\n",
    "        # get the number of model parameters\n",
    "        best_model_dict = load_torch_data(best_model_path)\n",
    "        config_dict = best_model_dict[\"config_dict\"]\n",
    "        model_config = config_dict.architecture\n",
    "        model = factory.create_model(model_config)\n",
    "        num_parameter = utils.get_model_parameters(model)\n",
    "        # parse rng seed\n",
    "        seed = re.search(r\"seed(\\d+)\", study_dir).group(1)\n",
    "        # load the data from the tensorboard log and validation loss file\n",
    "        best_val_loss = float(np.loadtxt(val_loss_path))\n",
    "        tb_log = load_tb_scalars(study_dir_path)\n",
    "        val_loss = tb_log.get(\"val/loss\", [])\n",
    "        min_idx = np.argmin(val_loss)\n",
    "        best_eps_loss = tb_log.get(\"val/eps\", [])[min_idx]\n",
    "        best_drude_loss = tb_log.get(\"val/drude\", [])[min_idx]\n",
    "        result_entry = {\n",
    "            \"seed\": seed,\n",
    "            \"num_parameter\": num_parameter,\n",
    "            \"lr\": config_dict.optimizer[\"lr\"],\n",
    "            \"val_loss\": best_val_loss,\n",
    "            \"eps_loss\": best_eps_loss,\n",
    "            \"drude_loss\": best_drude_loss,\n",
    "        }\n",
    "        # insert the data into the nested structure\n",
    "        if mode == \"spectra_dim\":\n",
    "            width = re.search(r\"hidden(\\d+)\", study_dir).group(1)\n",
    "            spectra_multiplier = re.search(r\"spectra_multiplier(\\d+\\.?\\d*)\", study_dir).group(1)\n",
    "            results.setdefault(width, {}).setdefault(spectra_multiplier, []).append(result_entry)\n",
    "        elif mode == \"mlp_addnorm\":\n",
    "            residual = \"no_residual\" if \"no_residual\" in study_dir else \"residual\"\n",
    "            num_mp_layer = re.search(r\"mp(\\d+\\.?\\d*)\", study_dir).group(1)\n",
    "            results.setdefault(residual, {}).setdefault(num_mp_layer, []).append(result_entry)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mode: {mode:s}\")\n",
    "    return results\n",
    "\n",
    "def fmt_param(v: float, decimals_if_needed: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Parameter count formatter function.\n",
    "    \"\"\"\n",
    "    if not np.isfinite(v):\n",
    "        return \"---\"\n",
    "    v = float(v)\n",
    "    if v >= 1_000_000:\n",
    "        val, unit = v / 1_000_000.0, \"M\"\n",
    "    elif v >= 1_000:\n",
    "        val, unit = v / 1_000.0, \"k\"\n",
    "    else:\n",
    "        return f\"{int(round(v)):d}\"\n",
    "    s = f\"{val:.{decimals_if_needed:d}f}\"\n",
    "    return f\"{s:s}{unit:s}\"\n",
    "\n",
    "def fmt_loss(m: float, s: float) -> str:\n",
    "    \"\"\"\n",
    "    Formatter function for loss with standard deviation.\n",
    "    \"\"\"\n",
    "    return rf\"${m:.3f} \\pm {s:.3f}$\" if np.isfinite(m) and np.isfinite(s) else \"---\"\n",
    "\n",
    "def power_law_with_floor(x: float, alpha: float, x0: float, l_0: float) -> float:\n",
    "    \"\"\"\n",
    "    Parameter scaling power law function.\n",
    "    \"\"\"\n",
    "    return l_0 + (x0 / x) ** alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of the 'MLP+AddNorm' block on the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing the scaling law study\n",
    "study_path = \"/scratch/magr4985/MLP_AddNorm\"\n",
    "\n",
    "# directory to save the results\n",
    "output_dir = \"./other_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# check if the study path exists, else just load in the results already stored in JSON files\n",
    "json_path = os.path.join(output_dir, \"mlp_addnorm_results.json\")\n",
    "if os.path.exists(study_path) and not os.path.exists(json_path):\n",
    "    print(f\"Study path {study_path:s} exists, loading results from there\")\n",
    "    results = load_results(study_path, mode=\"mlp_addnorm\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "else:\n",
    "    print(f\"Loading results from JSON file\")\n",
    "    with open(json_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "# directory where to store the figures and tables\n",
    "fig_dir = \"./other_data/results\"\n",
    "os.makedirs(fig_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for the model trained without residual MLP+AddNorm\n",
    "data = results[\"no_residual\"]\n",
    "sort_idx = np.argsort([int(k) for k in data.keys()])\n",
    "keys_sorted = np.array(list(data.keys()))[sort_idx]\n",
    "x = np.array([int(k) for k in keys_sorted], dtype=float)\n",
    "num_parameter_no_residual = np.array([data[k][0][\"num_parameter\"] for k in keys_sorted], dtype=float)\n",
    "y_mean_no_residual = np.array([np.mean([entry[\"val_loss\"] for entry in data[k]]) for k in keys_sorted], dtype=float)\n",
    "y_std_no_residual = np.array([np.std([entry[\"val_loss\"]  for entry in data[k]]) for k in keys_sorted], dtype=float)\n",
    "\n",
    "# prepare the data for the model trained with residual MLP+AddNorm\n",
    "data = results[\"residual\"]\n",
    "sort_idx = np.argsort([int(k) for k in data.keys()])\n",
    "keys_sorted = np.array(list(data.keys()))[sort_idx]\n",
    "x = np.array([int(k) for k in keys_sorted], dtype=float)\n",
    "num_parameter = np.array([data[k][0][\"num_parameter\"] for k in keys_sorted], dtype=float)\n",
    "y_mean = np.array([np.mean([entry[\"val_loss\"] for entry in data[k]]) for k in keys_sorted], dtype=float)\n",
    "y_std = np.array([np.std([entry[\"val_loss\"]  for entry in data[k]]) for k in keys_sorted], dtype=float)\n",
    "\n",
    "# make a latex table\n",
    "col_labels = [f\"{int(v)}\" for v in x]\n",
    "params_no_residual = [fmt_param(v) for v in num_parameter_no_residual]\n",
    "losses_no_residual = [f\"{v:.3f}\" for v in y_mean_no_residual]\n",
    "stds_no_residual = [f\"{v:.3f}\" for v in y_std_no_residual]\n",
    "params = [fmt_param(v) for v in num_parameter]\n",
    "losses = [f\"{v:.3f}\" for v in y_mean]\n",
    "stds = [f\"{v:.3f}\" for v in y_std]\n",
    "row_index = pd.MultiIndex.from_product(\n",
    "    [[\"MP-only\", r\"MP with MLP+\\textsc{Add\\&Norm}\"], [r\"$N$\", r\"$L_\\mathrm{val}$\", r\"$\\sigma(L_\\mathrm{val})$\"]],\n",
    "    names=[\"architecture\", \"metric\"],\n",
    ")\n",
    "col_index = pd.MultiIndex.from_arrays(\n",
    "    [[r\"$N_\\mathrm{MP}$\"] * len(col_labels), col_labels]\n",
    ")\n",
    "df = pd.DataFrame(\n",
    "    [params_no_residual, losses_no_residual, stds_no_residual, params, losses, stds],\n",
    "    index=row_index,\n",
    "    columns=col_index,\n",
    ")\n",
    "latex = df.to_latex(\n",
    "    escape=False,\n",
    "    index_names=False,\n",
    "    multirow=True,\n",
    "    multicolumn=True,\n",
    "    multicolumn_format=\"c\",\n",
    "    column_format=r\"c@{\\hspace{1em}}l@{\\hspace{1em}}\" + r\"c@{\\hspace{1em}}\"*len(col_labels),\n",
    ")\n",
    "latex = re.sub(r\"\\\\cline{1-10}\", r\"\\\\midrule\", latex)\n",
    "print(\"MLP+AddNorm Table:\")\n",
    "print(latex)\n",
    "with open(os.path.join(fig_dir, \"mlp_addnorm.txt\"), \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.errorbar(\n",
    "    x=x,\n",
    "    y=y_mean_no_residual,\n",
    "    yerr=y_std_no_residual,\n",
    "    fmt=\"o\",\n",
    "    markersize=4,\n",
    "    markeredgecolor=\"tab:orange\",\n",
    "    markerfacecolor=\"tab:orange\",\n",
    "    ecolor=\"tab:orange\",\n",
    "    capsize=4,\n",
    "    linestyle=\"none\",\n",
    ")\n",
    "ax.plot(x, y_mean_no_residual, \"-\", color=\"tab:orange\", label=\"MP-only\")\n",
    "ax.errorbar(\n",
    "    x=x,\n",
    "    y=y_mean,\n",
    "    yerr=y_std,\n",
    "    fmt=\"o\",\n",
    "    markersize=4,\n",
    "    markeredgecolor=\"tab:blue\",\n",
    "    markerfacecolor=\"tab:blue\",\n",
    "    ecolor=\"tab:blue\",\n",
    "    capsize=4,\n",
    "    linestyle=\"none\",\n",
    ")\n",
    "ax.plot(x, y_mean, \"-\", color=\"tab:blue\", label=r\"MP with MLP+\\textsc{Add\\&Norm}\")\n",
    "ax.set_xticks(list(range(1,9)))\n",
    "ax.set_ylim([1.09, 1.54])\n",
    "ax.set_xlabel(r\"$N_\\mathrm{MP}$\")\n",
    "ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "ax.legend(handlelength=1.25)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(fig_dir, \"mlp_addnorm.pdf\"))\n",
    "\n",
    "# quanitfy the improvement\n",
    "print(f\"Mean improvement   = {100 * np.mean(1 - (y_mean / y_mean_no_residual)):.2f}%\")\n",
    "print(f\"Median improvement = {100 * np.median(1 - (y_mean / y_mean_no_residual)):.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of the 'spectra_dim' on the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing the scaling law study\n",
    "study_path = \"/scratch/magr4985/Spectra_Dim\"\n",
    "\n",
    "# directory to save the results\n",
    "output_dir = \"./other_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# check if the study path exists, else just load in the results already stored in JSON files\n",
    "json_path = os.path.join(output_dir, \"spectra_dim_results.json\")\n",
    "if os.path.exists(study_path) and not os.path.exists(json_path):\n",
    "    print(f\"Study path {study_path:s} exists, loading results from there\")\n",
    "    results = load_results(study_path, mode=\"spectra_dim\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "else:\n",
    "    print(f\"Loading results from JSON file\")\n",
    "    with open(json_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "# directory where to store the figures and tables\n",
    "fig_dir = \"./other_data/results\"\n",
    "os.makedirs(fig_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "data = results[\"256\"] # used a fixed hidden dimension\n",
    "sort_idx = np.argsort([int(k) for k in data.keys()])\n",
    "keys_sorted = np.array(list(data.keys()))[sort_idx]\n",
    "x = np.array([int(k) for k in keys_sorted], dtype=float)\n",
    "num_parameter = np.array([data[k][0][\"num_parameter\"] for k in keys_sorted], dtype=float)\n",
    "y_mean = np.array([np.mean([entry[\"val_loss\"] for entry in data[k]]) for k in keys_sorted], dtype=float)\n",
    "y_std = np.array([np.std([entry[\"val_loss\"]  for entry in data[k]]) for k in keys_sorted], dtype=float)\n",
    "\n",
    "# make a latex table\n",
    "col_labels = [f\"{int(v)}\" for v in x]\n",
    "params = [fmt_param(v) for v in num_parameter]\n",
    "losses = [f\"{v:.3f}\" for v in y_mean]\n",
    "stds = [f\"{v:.3f}\" for v in y_std] \n",
    "df = pd.DataFrame(\n",
    "    [params, losses, stds],\n",
    "    index=[r\"$N$\", r\"$L_\\mathrm{val}$\", r\"$\\sigma(L_\\mathrm{val})$\"],\n",
    "    columns=pd.MultiIndex.from_arrays([[r\"$m_\\mathrm{spectra}$\"]*len(col_labels), col_labels])\n",
    ")\n",
    "latex = df.to_latex(multicolumn=True, multicolumn_format=\"c\", column_format=r\"l@{\\hspace{1em}}\" + r\"c@{\\hspace{1em}}\"*len(col_labels))\n",
    "print(\"Spectra Dimension Table:\")\n",
    "print(latex)\n",
    "with open(os.path.join(fig_dir, \"spectra_dim.txt\"), \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "# parameter scaling law parameters from \"scaling_results.ipynb\"\n",
    "alpha = 0.58\n",
    "x0 = 10**4.33\n",
    "l_0 = 1.03\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "plt.plot(\n",
    "    [x[0], x[-1]], \n",
    "    [\n",
    "        power_law_with_floor(num_parameter[0], alpha=alpha, x0=x0, l_0=l_0),\n",
    "        power_law_with_floor(num_parameter[-1], alpha=alpha, x0=x0, l_0=l_0),\n",
    "    ],\n",
    "    \"-\",\n",
    "    color=\"tab:orange\",\n",
    "    label=r\"$L(N)$\",\n",
    ")\n",
    "ax.errorbar(\n",
    "    x=x,\n",
    "    y=y_mean,\n",
    "    yerr=y_std,\n",
    "    fmt=\"o\",\n",
    "    markersize=4,\n",
    "    markeredgecolor=\"tab:blue\",\n",
    "    markerfacecolor=\"tab:blue\",\n",
    "    ecolor=\"tab:blue\",\n",
    "    capsize=4,\n",
    "    linestyle=\"none\",\n",
    "    label=r\"Data\"\n",
    ")\n",
    "ax.plot(x, y_mean, \"-\", color=\"tab:blue\")\n",
    "ax.set_xticks(list(range(1,9)))\n",
    "ax.set_xlabel(r\"$m_\\mathrm{Spectra}$\")\n",
    "ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "ax.legend(handlelength=1.25)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(fig_dir, \"spectra_dim.pdf\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
