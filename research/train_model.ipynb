{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Notebook version of my training script\n",
                "\n",
                "This is just for testing locally to ensure everything works properly before moving it to the cluster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "Run 'research/db_init.py' first.\n",
                "\n",
                "Train a model based on a configuration file (JSON file containing a dictionary).\n",
                "\n",
                "The configuration file contains training settings:\n",
                "{\n",
                "    \"seed\": int                 # Random seed\n",
                "    \"trial_dir\": str            # Path to the directory where the results will be saved (will be created if it does not exist)\n",
                "    \"num_train_data\": int       # Number of training data points (selected as a random subset)\n",
                "    \"batch_size\": int           # Batch size\n",
                "    \"architecture\": dict        # See 'optimetal.factory.model'\n",
                "    \"optimizer\": dict           # See 'optimetal.factory.optim'\n",
                "    \"loss_fn_eps\": str          # Loss function for the dielectric function, see 'optimetal.factory.loss'\n",
                "    \"loss_fn_drude\": str        # Loss function for the Drude frequency, see 'optimetal.factory.loss'\n",
                "    \"lr_scheduler\": str         # See 'optimetal.factory.lr_scheduler'\n",
                "    \"warmup_epochs\": int        # Number of learning rate warmup epochs\n",
                "    \"grad_clip\": float          # Gradient norm clipping threshold\n",
                "    \"eps_weight\": float         # Interband dielectric function loss weight\n",
                "    \"drude_weight\": float       # Drude frequency loss weight\n",
                "    \"early_stopping\": bool      # Flag to activate early stopping\n",
                "    \"patience\": int             # Early stopping patience (requires 'early_stopping=true')\n",
                "    \"num_epoch\": int            # Number of training epochs\n",
                "    \"precision\": str            # Automatic mixed precision training (\"auto\", \"bf16\" or \"fp32\")\n",
                "}\n",
                "\n",
                "Example configuration:\n",
                "    ./scaling_law_base_config/2b_variant2_hestness_data20000_seed42.json\n",
                "    \n",
                "The configuration dictionary is valid using pydantic. The architecture, learning rate scheduler, \n",
                "and optimizer dictionaries, however, are validated in their respective factories and model definitions.\n",
                "\n",
                "Track the training progress through something like this (adjust the path accordingly):\n",
                "    tensorboard --logdir /scratch/magr4985/Scaling_Base/2b_variant2_hestness_data20000_seed42\n",
                "https://docs.pytorch.org/docs/stable//tensorboard.html\n",
                "\"\"\"\n",
                "\n",
                "\"\"\"\n",
                "-----------------------------------------------------------------------------------------------------------------------\n",
                "START OF USER INPUT:\n",
                "\"\"\"\n",
                "\n",
                "# path to a configuration file that contains all training settings\n",
                "# (This configuration runs well on a laptop with a Nvidia RTX 3060 GPU)\n",
                "config_path = \"./scaling_law_base_config/2b_variant2_hestness_data20000_seed42.json\"\n",
                "\n",
                "# path to the training graphs\n",
                "train_path = \"../graph/train.pt\"\n",
                "\n",
                "# path to the validation graphs\n",
                "val_path = \"../graph/val.pt\"\n",
                "\n",
                "# device index (0 for GPU 0, 1 for GPU 1, etc. or -1 for CPU)\n",
                "device_index = 0\n",
                "\n",
                "\"\"\"\n",
                "END OF USER INPUT:\n",
                "-----------------------------------------------------------------------------------------------------------------------\n",
                "\"\"\"\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "from pydantic import TypeAdapter\n",
                "\n",
                "import optimetal.factory as factory\n",
                "from optimetal.training import Trainer\n",
                "from optimetal.data.loader import load_torch_data, create_dataloader\n",
                "from optimetal.utils import ValidateConfigurationDict, get_device, print_model_parameters, print_dict\n",
                "\n",
                "if not os.path.exists(train_path):\n",
                "    sys.exit(\"The path 'train_path' does not exist (training data not found)\")\n",
                "if not os.path.exists(val_path):\n",
                "    sys.exit(\"The path 'val_path' does not exist (validation data not found)\")\n",
                "if not os.path.exists(config_path):\n",
                "    sys.exit(\"The path 'config_path' does not exist\")\n",
                "    \n",
                "# load and validate the configuration dictionary\n",
                "with open(config_path, \"r\") as f:\n",
                "    config_dict = json.load(f)\n",
                "print(f\"Parameters from configuration file: {config_path:s}\")\n",
                "print_dict(config_dict) # debugging\n",
                "\n",
                "\"\"\"\n",
                "------------------------------------------------------\n",
                "HERE, WE ADJUST THE 'trial_dir' FOR TESTING PURPOSES\n",
                "------------------------------------------------------\n",
                "\"\"\"\n",
                "config_dict[\"trial_dir\"] = \"./test_trial\"\n",
                "\"\"\"\n",
                "------------------------------------------------------\n",
                "\"\"\"\n",
                "\n",
                "config_dict = TypeAdapter(ValidateConfigurationDict).validate_python(config_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load the datasets\n",
                "train_data = load_torch_data(train_path)\n",
                "val_data = load_torch_data(val_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create dataloader objects\n",
                "train_loader = create_dataloader(\n",
                "    train_data, \n",
                "    num_data=config_dict.num_train_data, \n",
                "    batch_size=config_dict.batch_size, \n",
                "    shuffle=True, # shuffle the training set\n",
                "    seed=config_dict.seed,\n",
                ")\n",
                "print(f\"Using {len(train_loader.dataset):d} graph from the training set\")\n",
                "val_loader = create_dataloader(\n",
                "    val_data, \n",
                "    num_data=-1, # use the whole validation dataset \n",
                "    batch_size=config_dict.batch_size, \n",
                "    shuffle=False, # do not shuffle the validation set\n",
                "    seed=config_dict.seed, # not needed here, but set it anyway\n",
                ")\n",
                "print(f\"Using the entire validation set\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# training device\n",
                "device = get_device(index=device_index)\n",
                "\n",
                "# build the model\n",
                "model_config = config_dict.architecture\n",
                "model = factory.create_model(model_config)\n",
                "\n",
                "# setup the optimizer\n",
                "optim_config = config_dict.optimizer\n",
                "optimizer = factory.create_optimizer(model, optim_config)\n",
                "\n",
                "# setup the loss functions for the dielectric function and Drude frequency\n",
                "loss_type_eps = config_dict.loss_fn_eps\n",
                "loss_fn_eps = factory.create_loss_fn(loss_type_eps)\n",
                "loss_type_drude = config_dict.loss_fn_drude\n",
                "loss_fn_drude = factory.create_loss_fn(loss_type_drude)\n",
                "\n",
                "# setup the learning rate scheduler\n",
                "lr_scheduler_config = config_dict.lr_scheduler\n",
                "lr_scheduler = factory.create_lr_scheduler(\n",
                "    optimizer, \n",
                "    lr_scheduler_config,\n",
                "    warmup_epochs=config_dict.warmup_epochs,\n",
                ")\n",
                "\n",
                "# build the trainer dictionary\n",
                "trainer_dict = {\n",
                "    \"config_dict\": config_dict, # metadata\n",
                "    \"trial_dir\": config_dict.trial_dir,\n",
                "    \"device\": device,\n",
                "    \"train_loader\": train_loader,\n",
                "    \"val_loader\": val_loader,\n",
                "    \"model\": model,\n",
                "    \"optimizer\": optimizer,\n",
                "    \"loss_fn_eps\": loss_fn_eps,\n",
                "    \"loss_fn_drude\": loss_fn_drude,  \n",
                "    \"lr_scheduler\": lr_scheduler,\n",
                "    \"grad_clip\": config_dict.grad_clip,\n",
                "    \"eps_weight\": config_dict.eps_weight,\n",
                "    \"drude_weight\": config_dict.drude_weight,\n",
                "    \"early_stopping\": config_dict.early_stopping,\n",
                "    \"patience\": config_dict.patience,\n",
                "    \"seed\": config_dict.seed,\n",
                "    \"precision\": config_dict.precision,\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# print a summary of the trainable model parameters\n",
                "print_model_parameters(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# build the trainer and train the model\n",
                "trainer = Trainer(\n",
                "    trainer_dict=trainer_dict,\n",
                "    checkpoint_every=10,\n",
                "    best_model_start_epoch=10,\n",
                ")\n",
                "trainer.train(config_dict.num_epoch)\n",
                "\n",
                "# log the best validation loss\n",
                "with open(os.path.join(config_dict.trial_dir, \"val_loss.txt\"), \"w\") as f:\n",
                "    f.write(f\"{trainer.best_val_loss:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
