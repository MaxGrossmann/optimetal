{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling law analysis and visualization\n",
    "\n",
    "This notebook has been written to run on the TU Ilmenau cluster with my specific setup.\n",
    "However, it can also be used simply to visualize the scaling laws. \n",
    "A summary of all results is also stored in the directory `/research/scaling_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Any\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import curve_fit\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.ticker import FixedLocator, ScalarFormatter\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "import torch_geometric\n",
    "\n",
    "from optimetal.evaluation import Evaluator\n",
    "from optimetal.data.loader import load_torch_data, create_dataloader\n",
    "\n",
    "def init_empty_results() -> dict:\n",
    "    \"\"\"\n",
    "    Initialize an empty 'results' structure for the scaling law study.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"data\": {\n",
    "            \"optimate\": {},\n",
    "            \"2b\": {\n",
    "                \"variant1\": {},\n",
    "                \"variant2\": {},\n",
    "            },\n",
    "            \"3b\": {},\n",
    "        },\n",
    "        \"parameter\": {\n",
    "            \"2b\": {\n",
    "                \"variant1\": {},\n",
    "                \"variant2\": {},\n",
    "            },\n",
    "            \"3b\": {},\n",
    "        },\n",
    "    }\n",
    "\n",
    "def init_empty_results_grid() -> dict:\n",
    "    \"\"\"\n",
    "    Initialize an empty 'results' structure for the scaling law grid study.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"2b\": {},\n",
    "        \"3b\": {},\n",
    "    }\n",
    "\n",
    "def extract_processed_dirs(results: dict) -> set[str]:\n",
    "    \"\"\"\n",
    "    Recursively walk all leaf lists in results and collect their 'study_dir'.\n",
    "    \"\"\"\n",
    "    processed = set()\n",
    "    def _walk(obj: Any) -> None:\n",
    "        if isinstance(obj, dict):\n",
    "            for v in obj.values():\n",
    "                _walk(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for entry in obj:\n",
    "                if \"study_dir\" in entry:\n",
    "                    processed.add(entry[\"study_dir\"])\n",
    "\n",
    "    _walk(results)\n",
    "    return processed\n",
    "\n",
    "def load_tb_scalars(logdir: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load scalar values from tensorboard event files. This is useful\n",
    "    when you want to look at training and validation loss curves.\n",
    "    \"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        logdir,\n",
    "        size_guidance={event_accumulator.SCALARS: 0},\n",
    "    )\n",
    "    ea.Reload()\n",
    "    tags = ea.Tags().get(\"scalars\", [])\n",
    "    tb_log = {}\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        values = [e.value for e in events]\n",
    "        tb_log[tag] = values\n",
    "    return tb_log\n",
    "\n",
    "def eval_model(\n",
    "    best_model_path: str, \n",
    "    dataloader: torch_geometric.loader.DataLoader,\n",
    "    device_index=0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Use the Evaluator class to gather metrics for a given model.\n",
    "    \"\"\"\n",
    "    evaluator = Evaluator(\n",
    "        best_model_path=best_model_path, \n",
    "        dataloader=dataloader, \n",
    "        device_index=device_index,\n",
    "        turn_off_progress_bar=True,\n",
    "    )\n",
    "    num_parameter = evaluator.num_parameter\n",
    "    evaluator.evaluate()\n",
    "    metric_dict = {\n",
    "        \"mean_metrics\": evaluator.mean_metrics,\n",
    "        \"median_metrics\": evaluator.median_metrics,\n",
    "        \"std_metrics\": evaluator.std_metrics,\n",
    "        \"drude_r2\": evaluator.drude_r2,\n",
    "    }\n",
    "    return num_parameter, metric_dict\n",
    "    \n",
    "def process_one(\n",
    "        study_path: str,\n",
    "        study_dir: str,\n",
    "        results: dict,\n",
    "        dataloader: torch_geometric.loader.DataLoader,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Load and evaluate a single 'study_dir', then insert its entry into the right place in 'results'.\n",
    "    Input:\n",
    "        study_path:     Path to the root directory containing study result subdirectories\n",
    "        study_dir:      Name of the study directory in 'study_path'\n",
    "        results:        Nested dict mapping study types and hyperparameters to dictionaries\n",
    "        dataloader:     Used to evaluate the best model in the study directory\n",
    "    Output:\n",
    "        results:        Nested dict mapping study types and hyperparameters to dictionaries \n",
    "                        with 'best_val_loss', 'train_loss', and 'val_loss'\n",
    "    \"\"\"\n",
    "    # path setup and checks\n",
    "    study_dir_path = os.path.join(study_path, study_dir)\n",
    "    val_loss_path = os.path.join(study_dir_path, \"val_loss.txt\")\n",
    "    best_model_path = os.path.join(study_dir_path, \"best_model.pt\")\n",
    "    if not os.path.exists(val_loss_path) or not os.path.exists(best_model_path):\n",
    "        print(f\"Skipping {study_dir_path:s}, probably still running\")\n",
    "        return\n",
    "    # parse the seed (metadata)\n",
    "    seed = int(re.search(r\"seed(\\d+)\", study_dir).group(1))\n",
    "    # load the data from the tensorboard log and validation loss file\n",
    "    best_val_loss = float(np.loadtxt(val_loss_path))\n",
    "    tb_log = load_tb_scalars(study_dir_path)\n",
    "    val_loss = tb_log.get(\"val/loss\", [])\n",
    "    min_idx = np.argmin(val_loss)\n",
    "    best_eps_loss = tb_log.get(\"val/eps\", [])[min_idx]\n",
    "    best_drude_loss = tb_log.get(\"val/drude\", [])[min_idx]\n",
    "    result_entry = {\n",
    "        \"study_dir\": study_dir,\n",
    "        \"seed\": seed,\n",
    "        \"val_loss\": best_val_loss,\n",
    "        \"eps_loss\": best_eps_loss,\n",
    "        \"drude_loss\": best_drude_loss,\n",
    "    }\n",
    "    # use the \"Evaluator\" class to evaluate the best model and obtain more detailed metrics\n",
    "    # (this can take some time...)\n",
    "    num_parameter, metric_dict = eval_model(\n",
    "        best_model_path=best_model_path,\n",
    "        dataloader=dataloader,\n",
    "    )\n",
    "    result_entry[\"num_parameter\"] = num_parameter\n",
    "    result_entry[\"metric_dict\"] = metric_dict\n",
    "    # data scaling\n",
    "    if \"hestness\" in study_dir:\n",
    "        num_data = re.search(r\"data(\\d+)\", study_dir).group(1)\n",
    "        if \"optimate\" in study_dir:\n",
    "            results[\"data\"][\"optimate\"].setdefault(num_data, []).append(result_entry)\n",
    "        elif \"2b\" in study_dir:\n",
    "            if \"variant1\" in study_dir:\n",
    "                results[\"data\"][\"2b\"][\"variant1\"].setdefault(num_data, []).append(result_entry)\n",
    "            elif \"variant2\" in study_dir:\n",
    "                results[\"data\"][\"2b\"][\"variant2\"].setdefault(num_data, []).append(result_entry)\n",
    "        elif \"3b\" in study_dir:\n",
    "            results[\"data\"][\"3b\"].setdefault(num_data, []).append(result_entry)\n",
    "    # parameter scaling\n",
    "    elif \"kaplan\" in study_dir:\n",
    "        width = re.search(r\"width(\\d+)\", study_dir).group(1)\n",
    "        if \"2b\" in study_dir:\n",
    "            if \"variant1\" in study_dir:\n",
    "                results[\"parameter\"][\"2b\"][\"variant1\"].setdefault(width, []).append(result_entry)\n",
    "            elif \"variant2\" in study_dir:\n",
    "                results[\"parameter\"][\"2b\"][\"variant2\"].setdefault(width, []).append(result_entry)\n",
    "        elif \"3b\" in study_dir:\n",
    "            results[\"parameter\"][\"3b\"].setdefault(width, []).append(result_entry)\n",
    "\n",
    "def process_one_grid(\n",
    "        study_path: str,\n",
    "        study_dir: str,\n",
    "        results: dict,\n",
    "        dataloader: torch_geometric.loader.DataLoader,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Load and evaluate a single 'study_dir', then insert its entry into the right place in 'results'.\n",
    "    This version is for the scaling law grid study, where the data and parameter scaling are combined.\n",
    "    Input:\n",
    "        study_path:     Path to the root directory containing study result subdirectories\n",
    "        study_dir:      Name of the study directory in 'study_path'\n",
    "        results:        Nested dict mapping study types and hyperparameters to dictionaries\n",
    "        dataloader:     Used to evaluate the best model in the study directory\n",
    "    Output:\n",
    "        results:        Nested dict mapping study types and hyperparameters to dictionaries \n",
    "                        with 'best_val_loss', 'train_loss', and 'val_loss'\n",
    "    \"\"\"\n",
    "    # path setup and checks\n",
    "    study_dir_path = os.path.join(study_path, study_dir)\n",
    "    val_loss_path = os.path.join(study_dir_path, \"val_loss.txt\")\n",
    "    best_model_path = os.path.join(study_dir_path, \"best_model.pt\")\n",
    "    if not os.path.exists(val_loss_path) or not os.path.exists(best_model_path):\n",
    "        print(f\"Skipping {study_dir_path:s}, probably still running\")\n",
    "        return\n",
    "    # parse the seed (metadata)\n",
    "    seed = int(re.search(r\"seed(\\d+)\", study_dir).group(1))\n",
    "    # load the data from the tensorboard log and validation loss file\n",
    "    best_val_loss = float(np.loadtxt(val_loss_path))\n",
    "    tb_log = load_tb_scalars(study_dir_path)\n",
    "    val_loss = tb_log.get(\"val/loss\", [])\n",
    "    min_idx = np.argmin(val_loss)\n",
    "    best_eps_loss = tb_log.get(\"val/eps\", [])[min_idx]\n",
    "    best_drude_loss = tb_log.get(\"val/drude\", [])[min_idx]\n",
    "    result_entry = {\n",
    "        \"study_dir\": study_dir,\n",
    "        \"seed\": seed,\n",
    "        \"val_loss\": best_val_loss,\n",
    "        \"eps_loss\": best_eps_loss,\n",
    "        \"drude_loss\": best_drude_loss,\n",
    "    }\n",
    "    # use the \"Evaluator\" class to evaluate the best model and obtain more detailed metrics\n",
    "    # (this can take some time...)\n",
    "    num_parameter, metric_dict = eval_model(\n",
    "        best_model_path=best_model_path,\n",
    "        dataloader=dataloader,\n",
    "    )\n",
    "    result_entry[\"num_parameter\"] = num_parameter\n",
    "    result_entry[\"metric_dict\"] = metric_dict\n",
    "    # put the result entry into the right place in the results dictionary\n",
    "    num_data = re.search(r\"data(\\d+)\", study_dir).group(1)\n",
    "    width = re.search(r\"width(\\d+)\", study_dir).group(1)\n",
    "    if \"2b\" in study_dir:\n",
    "        results[\"2b\"].setdefault(num_data, {}).setdefault(width, []).append(result_entry)\n",
    "    elif \"3b\" in study_dir:\n",
    "        results[\"3b\"].setdefault(num_data, {}).setdefault(width, []).append(result_entry)\n",
    "\n",
    "def _filter_nonempty(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Return only keys whose value list is non-empty.\n",
    "    \"\"\"\n",
    "    return {k: v for k, v in data.items() if v}\n",
    "\n",
    "def _regroup_by_entry_key(data: dict, x_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a dictionary that is keyed by hyperparameters into a dictionary that is keyed by entries.\n",
    "    This is useful when you want the x-axis to display a value stored in each entry. For example, you could use 'num_parameter'.\n",
    "    \"\"\"\n",
    "    regroup = {}\n",
    "    for _, entries in data.items():\n",
    "        for entry in entries:\n",
    "            x_val = entry[x_key]\n",
    "            regroup.setdefault(x_val, []).append(entry)\n",
    "    return regroup\n",
    "\n",
    "def plot_scaling_law(\n",
    "    ax: plt.Axes, \n",
    "    data: dict, \n",
    "    label: str, \n",
    "    color: str,\n",
    "    key: str = \"val_loss\", \n",
    "    x_from_entry: str | None = None,\n",
    "    error_bars = False,\n",
    "    ms: int = 4,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot one scaling law curve.\n",
    "    \"\"\"\n",
    "    data = _filter_nonempty(data)\n",
    "    if x_from_entry is not None:\n",
    "        data = _regroup_by_entry_key(data, x_from_entry)\n",
    "    if len(data) < 2:\n",
    "        raise ValueError(\"Need at least two points for a fit\")\n",
    "    sort_idx = np.argsort([int(x) for x in data.keys()])\n",
    "    x = np.array(sorted([int(x) for x in data.keys()]), dtype=float)\n",
    "    y_mean = np.array(\n",
    "        [np.mean([entry[key] for entry in data[k]]) for k in np.array(list(data.keys()))[sort_idx]], \n",
    "        dtype=float,\n",
    "    )\n",
    "    if error_bars:\n",
    "        y_std = np.array(\n",
    "            [np.std([entry[key] for entry in data[k]]) for k in np.array(list(data.keys()))[sort_idx]], \n",
    "            dtype=float,\n",
    "        )\n",
    "        ax.errorbar(\n",
    "            x,\n",
    "            y_mean,\n",
    "            yerr=y_std,\n",
    "            fmt=\"o\",\n",
    "            markersize=ms,\n",
    "            markeredgecolor=color,\n",
    "            markerfacecolor=color,\n",
    "            ecolor=color,\n",
    "            capsize=4,\n",
    "            linestyle=\"none\",\n",
    "            label=label,\n",
    "        ) \n",
    "    else:\n",
    "        ax.plot(x, y_mean, \"o\", markersize=ms, label=label, color=color)\n",
    "\n",
    "\"\"\"\n",
    "Scaling law function block.\n",
    "######################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "def power_law(x: float, alpha: float, x0: float) -> float:\n",
    "    return (x0 / x) ** alpha\n",
    "\n",
    "def power_law_with_floor(x: float, alpha: float, x0: float, l_0: float) -> float:\n",
    "    return l_0 + (x0 / x) ** alpha\n",
    "\n",
    "def broken_power_law(x: float, alpha1: float, alpha2: float, xc: float) -> float:\n",
    "    return ((xc/ x) ** (alpha2)) * ((1 + (xc / x)) ** (alpha1 - alpha2))\n",
    "\n",
    "def broken_power_law_with_amp(x: float, alpha1: float, alpha2: float, xc: float, A: float) -> float:\n",
    "    return A * ((xc/ x) ** (alpha2)) * ((1 + (xc / x)) ** (alpha1 - alpha2))\n",
    "\n",
    "\"\"\"\n",
    "######################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "def calc_aicc(residuals: np.ndarray, k: int) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    https://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_least_squares\n",
    "    \"\"\"\n",
    "    residuals = np.asarray(residuals, dtype=float)\n",
    "    n = residuals.size\n",
    "    rss = np.sum(residuals**2)\n",
    "    sigma2_hat = rss / n\n",
    "    aic = 2 * k + n * np.log(sigma2_hat)\n",
    "    aicc = aic + ((2 * k ** 2 + 2 * k) / (n - k - 1))\n",
    "    return aicc\n",
    "\n",
    "def fit_scaling_law(\n",
    "    data: dict, \n",
    "    func_type: str, # see below\n",
    "    key: str = \"best_val_loss\",\n",
    "    x_from_entry: str | None = None,\n",
    ")-> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Fit the specified power law function to the scaling data.\n",
    "    \"\"\"\n",
    "    data = _filter_nonempty(data)\n",
    "    if x_from_entry is not None:\n",
    "        data = _regroup_by_entry_key(data, x_from_entry)\n",
    "    if len(data) < 2:\n",
    "        raise ValueError(\"Need at least two points for a fit\")\n",
    "    sort_idx = np.argsort([int(x) for x in data.keys()])\n",
    "    x = np.array(sorted([int(x) for x in data.keys()]), dtype=float)\n",
    "    y_mean = np.array(\n",
    "        [np.mean([entry[key] for entry in data[k]]) for k in np.array(list(data.keys()))[sort_idx]], \n",
    "        dtype=float,\n",
    "    )\n",
    "    y_std = np.array(\n",
    "        [np.std([entry[key] for entry in data[k]]) for k in np.array(list(data.keys()))[sort_idx]], \n",
    "        dtype=float,\n",
    "    )\n",
    "    # select power law function type\n",
    "    if func_type == \"simple\":\n",
    "        func = power_law\n",
    "        alpha0 = 0.1 # typical starting value\n",
    "        x0_0 = x[0] # characteristic scale\n",
    "        p0 = [alpha0, x0_0]\n",
    "        bounds = ([-np.inf, -np.inf], [np.inf, np.inf])\n",
    "    elif func_type == \"floor\":\n",
    "        func = power_law_with_floor\n",
    "        alpha0 = 0.1 # typical starting value\n",
    "        x0_0 = x[0] # characteristic scale\n",
    "        l_00 = y_mean[-1] # smallest observed loss\n",
    "        p0 = [alpha0, x0_0, l_00]\n",
    "        bounds = ([-np.inf, -np.inf, 0.0], [np.inf, np.inf, np.inf])\n",
    "    elif func_type == \"broken\":\n",
    "        func = broken_power_law\n",
    "        alpha0 = 0.1 # typical starting value\n",
    "        xc_0 = x[0] # critical size, where scaling changes\n",
    "        p0 = [alpha0, alpha0, xc_0]\n",
    "        bounds = ([-np.inf, -np.inf, -np.inf], [np.inf, np.inf, np.inf])\n",
    "    elif func_type == \"broken_with_amp\":\n",
    "        func = broken_power_law_with_amp\n",
    "        alpha0 = 0.1 # typical starting value\n",
    "        xc_0 = x[0] # critical size, where scaling changes\n",
    "        A0 = 1.0 # our validation loss is around 1.0\n",
    "        p0 = [alpha0, alpha0, xc_0, A0]\n",
    "        bounds = ([-np.inf, -np.inf, -np.inf, -np.inf], [np.inf, np.inf, np.inf, np.inf])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported power law type\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        popt, pcov = curve_fit(\n",
    "            f=func,\n",
    "            xdata=x,\n",
    "            ydata=y_mean,\n",
    "            p0=p0,\n",
    "            sigma=y_std,\n",
    "            absolute_sigma=False, # default\n",
    "            bounds=bounds,\n",
    "            maxfev=10000,\n",
    "        )\n",
    "    y_fit = func(x, *popt)\n",
    "    res = y_mean - y_fit\n",
    "    aicc = calc_aicc(res, len(popt))\n",
    "    return {\n",
    "        \"popt\": popt, \n",
    "        \"pcov\": pcov, \n",
    "        \"aicc\": aicc,\n",
    "    }\n",
    "    \n",
    "def get_power_law(func_type: str) -> Callable[..., float]:\n",
    "    \"\"\"\n",
    "    Helper function to select power law functions from strings.\n",
    "    \"\"\"\n",
    "    if func_type == \"simple\":\n",
    "        func = power_law\n",
    "    elif func_type == \"floor\":\n",
    "        func = power_law_with_floor\n",
    "    elif func_type == \"broken\":\n",
    "        func = broken_power_law\n",
    "    elif func_type == \"broken_with_amp\":\n",
    "        func = broken_power_law_with_amp\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported power law type\")\n",
    "    return func\n",
    "\n",
    "def get_fit_func_str(func_type: str, X: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to get a latex equation of a power law functions.\n",
    "    \"\"\"\n",
    "    if func_type == \"simple\":\n",
    "        leg_str = rf\"$L({X:s}) = \\left({X:s}_0/{X:s}\\right)^{{\\alpha_{{{X:s}}}}}$\"\n",
    "    elif func_type == \"floor\":\n",
    "        leg_str = rf\"$L({X:s}) = L_\\infty + \\left({X:s}_0/{X:s}\\right)^{{\\alpha_{{{X:s}}}}}$\"\n",
    "    elif func_type == \"broken\":\n",
    "        leg_str = (rf\"$L({X:s}) = \\left({X:s}_c/{X:s}\\right)^{{\\alpha_{{{X:s}, 2}}}}\"\n",
    "                   rf\"\\left(1+{X:s}_c/{X:s}\\right)^{{\\alpha_{{{X:s}, 1}}-\\alpha_{{{X:s}, 2}}}}$\")\n",
    "    elif func_type == \"broken_with_amp\":\n",
    "        leg_str = (rf\"$L({X:s}) = A \\cdot \\left[\\left({X:s}_c/{X:s}\\right)^{{\\alpha_{{{X:s}, 2}}}}\"\n",
    "                   rf\"\\left(1+{X:s}_c/{X:s}\\right)^{{\\alpha_{{{X:s}, 1}}-\\alpha_{{{X:s}, 2}}}}\\right]$\")\n",
    "    elif func_type == \"kaplan\":\n",
    "        leg_str = (r\"$L(N, D) = \\left[\\left(\\frac{N_0}{N}\\right)^{\\frac{\\alpha_N}{\\alpha_D}} + \"\n",
    "                   r\"\\frac{D_0}{D}\\right]^{\\alpha_D}$\")\n",
    "    elif func_type == \"global\":\n",
    "        leg_str = (r\"$L(N, D) = \\left[\\left(\\frac{N_0}{N}\\right)^{\\frac{\\alpha_N}{\\alpha_D}} + \\left(\\frac{D_c}{D}\\right)\"\n",
    "                   r\"\\left(1 + \\frac{D_c}{D}\\right)^{\\frac{\\alpha_{D,1}}{\\alpha_{D,2}} - 1}\\right]^{\\alpha_{D,2}}$\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported power law type\")\n",
    "    return leg_str\n",
    "\n",
    "def format_val_err(val: float, err: float, digits: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for string formatting, used for latex tables.\n",
    "    \"\"\"\n",
    "    return f\"${val:.{digits:d}f} \\\\pm {err:.{digits:d}f}$\"\n",
    "\n",
    "def format_val_err_log10(x: float, s: float | None = None, digits: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Format large positive parameters with uncertainty in scientific notation.\n",
    "    \"\"\"\n",
    "    if x <= 0:\n",
    "        raise ValueError(\"x must be positive.\")\n",
    "    xl = np.log10(x)\n",
    "    if s is None:\n",
    "        return rf\"$10^{{{xl:.{digits:d}f}}}$\"\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"s must be positive.\")\n",
    "    sl = np.log10(s)\n",
    "    return rf\"$10^{{{xl:.{digits:d}f}}} \\pm 10^{{{sl:.{digits:d}f}}}$\"\n",
    "\n",
    "def build_rows_from_fits(fits: list, func_type: str, X: str = \"D\") -> dict:\n",
    "    \"\"\" \n",
    "    Helper function for building a dictionary from the fit results of the single scaling laws.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for name, popt, pcov, aicc in fits:\n",
    "        se = np.sqrt(np.diag(pcov))\n",
    "        # default fields\n",
    "        record = {\n",
    "            \"Model\": name, \n",
    "            \"Form\": func_type,\n",
    "            \"alpha\": None, \n",
    "            \"alpha1\": None, \n",
    "            \"alpha2\": None,\n",
    "            f\"{X:s}_0\": None, \n",
    "            f\"{X:s}_c\": None,\n",
    "            \"A\": None, \n",
    "            \"L_0\": None,\n",
    "            \"AICc\": f\"{aicc:.2f}\",\n",
    "        }\n",
    "        # populate depending on scaling law function type\n",
    "        if func_type == \"simple\":\n",
    "            alpha, x0 = popt\n",
    "            se_alpha, se_x0 = se\n",
    "            record[\"alpha\"] = format_val_err(alpha, se_alpha)\n",
    "            record[f\"{X:s}_0\"] = format_val_err_log10(x0, se_x0, digits=2)\n",
    "        elif func_type == \"floor\":\n",
    "            alpha, x0, linf = popt\n",
    "            se_alpha, se_x0, se_linf = se\n",
    "            record[\"alpha\"] = format_val_err(alpha, se_alpha)\n",
    "            record[f\"{X:s}_0\"] = format_val_err_log10(x0, se_x0, digits=2)\n",
    "            record[\"L_0\"] = format_val_err(linf, se_linf)\n",
    "        elif func_type == \"broken\":\n",
    "            a1, a2, xc = popt\n",
    "            se_a1, se_a2, se_xc = se\n",
    "            record[\"alpha1\"] = format_val_err(a1, se_a1)\n",
    "            record[\"alpha2\"] = format_val_err(a2, se_a2)\n",
    "            record[f\"{X:s}_c\"] = format_val_err_log10(xc, se_xc, digits=2)\n",
    "        elif func_type == \"broken_with_amp\":\n",
    "            a1, a2, xc, A = popt\n",
    "            se_a1, se_a2, se_xc, se_A = se\n",
    "            record[\"alpha1\"] = format_val_err(a1, se_a1)\n",
    "            record[\"alpha2\"] = format_val_err(a2, se_a2)\n",
    "            record[f\"{X:s}_c\"] = format_val_err_log10(xc, se_xc, digits=2)\n",
    "            record[\"A\"] = format_val_err(A, se_A)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported form: {func_type:s}\")\n",
    "        rows.append(record)\n",
    "    return rows\n",
    "\n",
    "def latex_table_from_rows(rows: list[dict], X: str, fit_func_display: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for building latex tables for the single scaling law fits.\n",
    "    \"\"\"\n",
    "    cols = [\n",
    "        \"Model\", \n",
    "        \"Form\",\n",
    "        \"alpha\", \n",
    "        \"alpha1\", \n",
    "        \"alpha2\",\n",
    "        f\"{X:s}_0\",\n",
    "        f\"{X:s}_c\",\n",
    "        \"A\", \n",
    "        \"L_0\", \n",
    "        \"AICc\",\n",
    "    ]\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    display_cols = [c for c in cols if c not in (\"Model\", \"Form\")]\n",
    "    df[display_cols] = df[display_cols].where(pd.notnull(df[display_cols]), \"---\")\n",
    "    keep_cols = [c for c in df.columns if not (df[c] == \"---\").all()]\n",
    "    df = df[keep_cols]\n",
    "    col_map = {\n",
    "        \"Model\": \"Model\",\n",
    "        \"Form\": \"Form\",\n",
    "        \"alpha\": rf\"$\\alpha_{{{X:s}}}$\",\n",
    "        \"alpha1\": rf\"$\\alpha_{{{X:s},1}}$\",\n",
    "        \"alpha2\": rf\"$\\alpha_{{{X:s},2}}$\",\n",
    "        f\"{X:s}_0\": rf\"${X:s}_0$\",\n",
    "        f\"{X:s}_c\": rf\"${X:s}_c$\",\n",
    "        \"A\": r\"$A$\",\n",
    "        \"L_0\": r\"$L_\\infty$\",\n",
    "        \"AICc\": \"AICc\",\n",
    "    }\n",
    "    df = df.rename(columns=col_map)\n",
    "    # redundant information\n",
    "    func_type = rows[0][\"Form\"]\n",
    "    if \"Form\" in df.columns:\n",
    "        df = df.drop(columns=[\"Form\"])\n",
    "    if fit_func_display:\n",
    "        # determine parameter columns\n",
    "        cols_order = list(df.columns)\n",
    "        if \"Model\" not in cols_order or \"AICc\" not in cols_order:\n",
    "            raise ValueError(\"Expected 'Model' and 'AICc' columns after processing\")\n",
    "        param_cols = [c for c in cols_order if c not in (\"Model\", \"AICc\")]\n",
    "        # build a MultiIndex for columns\n",
    "        # (top level: \"\" over Model, \"Forms\" over parameters, \"\" over AICc)\n",
    "        top = []\n",
    "        bottom = []\n",
    "        for c in [\"Model\"] + param_cols + [\"AICc\"]:\n",
    "            if c == \"Model\" or c == \"AICc\":\n",
    "                top.append(\"\")\n",
    "                bottom.append(c)\n",
    "            else:\n",
    "                top.append(get_fit_func_str(func_type, X))\n",
    "                bottom.append(c)\n",
    "\n",
    "        df = df[[\"Model\"] + param_cols + [\"AICc\"]]\n",
    "        df.columns = pd.MultiIndex.from_arrays([top, bottom])\n",
    "    latex = df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        longtable=False,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format=\"c\",\n",
    "        bold_rows=False,\n",
    "        column_format=len(df.columns) * r\"c@{\\hspace{1em}}\",\n",
    "    )\n",
    "    if fit_func_display:\n",
    "        lines = latex.splitlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip().startswith(\"Model\"):\n",
    "                lines.insert(i, rf\"\\cmidrule(lr){{2-{len(param_cols) + 1}}}\")\n",
    "                break\n",
    "        latex = \"\\n\".join(lines)\n",
    "    return latex\n",
    "\n",
    "def grid_to_vectors(\n",
    "    grid_dict: dict,\n",
    "    metric_key: str = \"val_loss\",\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Helper function that converts data for the scaling law L(D,N) into a more manageable format.\n",
    "    \"\"\"\n",
    "    data_keys = sorted([int(x) for x in grid_dict.keys()])\n",
    "    param_keys = sorted([int(x) for x in grid_dict[str(data_keys[0])].keys()])\n",
    "    datapoints, widths, num_parameters, mus, stds = [], [], [], [], []\n",
    "    num_d = len(data_keys)\n",
    "    num_n = len(param_keys)\n",
    "    for dk in data_keys:\n",
    "        by_n = grid_dict[str(dk)]\n",
    "        for wk in param_keys:\n",
    "            entries = by_n[str(wk)]\n",
    "            vals = np.array([e[metric_key] for e in entries])\n",
    "            datapoints.append(dk)\n",
    "            widths.append(wk)\n",
    "            num_parameters.append(entries[0][\"num_parameter\"])\n",
    "            mus.append(np.mean(vals))\n",
    "            stds.append(np.std(vals))\n",
    "    datapoints = np.array(datapoints).reshape(num_d, num_n)\n",
    "    widths = np.array(widths).reshape(num_d, num_n)\n",
    "    num_parameters = np.array(num_parameters).reshape(num_d, num_n)\n",
    "    mus = np.array(mus).reshape(num_d, num_n)\n",
    "    stds = np.array(stds).reshape(num_d, num_n)\n",
    "    return datapoints, widths, num_parameters, mus, stds\n",
    "    \n",
    "def fmt_param(v: float, digits: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Parameter count formatter function.\n",
    "    \"\"\"\n",
    "    if not np.isfinite(v):\n",
    "        return \"---\"\n",
    "    v = float(v)\n",
    "    if v >= 1_000_000:\n",
    "        val, unit = v / 1_000_000.0, r\"\\,M\"\n",
    "    elif v >= 1_000:\n",
    "        val, unit = v / 1_000.0, r\"\\,k\"\n",
    "    else:\n",
    "        return f\"{int(round(v)):d}\"\n",
    "    if np.isclose(val, np.round(val), rtol=0.0, atol=1e-12):\n",
    "        s = f\"{int(np.round(val)):d}\"\n",
    "    else:\n",
    "        s = f\"{val:.{digits:d}f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    return f\"{s:s}{unit:s}\"\n",
    "\n",
    "def global_scaling_fit_kaplan(X, alpha_n, alpha_d1, alpha_d2, Nc, Dc):\n",
    "    \"\"\"\n",
    "    Combination of broken data scaling and simple power law parameter scaling following the form of Kaplan et al.\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        D, N = X\n",
    "        return ((Nc / N) ** (alpha_n / alpha_d2) + (Dc / D) * (1 + Dc / D) ** (alpha_d1/alpha_d2 - 1)) ** alpha_d2\n",
    "    \n",
    "def global_scaling_fit_hoffmann(X, alpha_n, alpha_d1, alpha_d2, Nc, Dc, l_0):\n",
    "    \"\"\"\n",
    "    Combination of broken data scaling and simple power law parameter scaling following the form of Hoffmann et al.\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        D, N = X\n",
    "        return l_0 + (Nc / N) ** (alpha_n) + ((Dc / D) ** (alpha_d2)) * ((1 + Dc / D) ** (alpha_d1 - alpha_d2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the results from the \"line search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing the scaling law study\n",
    "study_path = \"/scratch/magr4985/Scaling_Base\"\n",
    "\n",
    "# directory to save the results\n",
    "output_dir = \"./scaling_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# directory where to store the figures\n",
    "fig_dir = \"./scaling_data/scaling\"\n",
    "os.makedirs(fig_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the all training runs converged\n",
    "if os.path.exists(study_path):\n",
    "    # load all study directories, valdation loss files, and tensorboard logs\n",
    "    study_dirs = [d for d in os.listdir(study_path) if os.path.isdir(os.path.join(study_path, d))]\n",
    "    print(f\"[INFO] Found {len(study_dirs):d} study directories in {study_path:s}\")\n",
    "    checks = []\n",
    "    for study_dir in study_dirs:\n",
    "        val_loss = None\n",
    "        if os.path.exists(os.path.join(study_path, study_dir, \"val_loss.txt\")):\n",
    "            val_loss = float(np.loadtxt(os.path.join(study_path, study_dir, \"val_loss.txt\")))\n",
    "        checks.append([study_dir, val_loss, load_tb_scalars(os.path.join(study_path, study_dir))])\n",
    "    # print the study directories with diverging gradients, i.e, where something went wrong\n",
    "    print(\"\\n[DIVERGING GRADIENTS CHECK]\")\n",
    "    for study_dir, val_loss, tb_log in checks:\n",
    "        if np.max(tb_log[\"train/grad_norm\"]) > 1e2:\n",
    "            print(\"[DIVERGING GRADIENTS] \", study_dir, np.max(tb_log[\"train/grad_norm\"]))\n",
    "    # print study directories with training not finished\n",
    "    print(\"\\n[TRAINING FINISHED CHECK]\")\n",
    "    for study_dir, val_loss, tb_log in checks:\n",
    "        if len(tb_log.get(\"val/loss\", [])) < 500:\n",
    "            print(\"[TRAINING NOT FINISHED] \", study_dir, len(tb_log.get(\"val/loss\", [])))\n",
    "    # seed-to-seed validation-loss consistency\n",
    "    print(\"\\n[SEED VARIANCE CHECK]\")\n",
    "    max_seed_variance_thr = 0.05\n",
    "    _seed_suffix_re = re.compile(r'([_-])seed\\d+$')\n",
    "    _seed_extract_re = re.compile(r'seed(\\d+)$')\n",
    "    def _key_without_seed(name: str) -> str:\n",
    "        return _seed_suffix_re.sub('', name)\n",
    "    def _seed_or_inf(name: str) -> int:\n",
    "        m = _seed_extract_re.search(name)\n",
    "        return int(m.group(1)) if m else 10**9 # for nice sorting when seed missing\n",
    "    groups = defaultdict(list)\n",
    "    for study_dir, val_loss, _ in checks:\n",
    "        if val_loss is not None and np.isfinite(val_loss):\n",
    "            groups[_key_without_seed(study_dir)].append((study_dir, float(val_loss)))\n",
    "    for cfg_key, items in groups.items():\n",
    "        if len(items) < 2:\n",
    "            continue # need at least two seeds to compare\n",
    "        vals = [v for _, v in items]\n",
    "        sorted_vals = sorted(vals)\n",
    "        low = sorted_vals[0]\n",
    "        mid = sorted_vals[len(sorted_vals) // 2]\n",
    "        high = sorted_vals[-1]\n",
    "        if low <= 0:\n",
    "            continue # avoid divide-by-zero/undefined relative difference\n",
    "        max_rel_spread = max((mid - low) / mid, (high - mid) / mid)\n",
    "        if max_rel_spread > max_seed_variance_thr:\n",
    "            print(f\"[SEED VARIANCE > 5%] {cfg_key:s}: max(spread)={100*max_rel_spread:.2f}% (low={low:.6g}, mid={mid:.6g} , high={high:.6g})\")\n",
    "            for sd, v in sorted(items, key=lambda p: _seed_or_inf(p[0])):\n",
    "                print(f\"    {sd:s}: val_loss={v:.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incrementally load and evaluate all models in the scaling-law study, caching results to disk.\n",
    "\n",
    "Evaluating every model can be time-consuming, so this process saves intermediate results\n",
    "to a JSON file and, on subsequent runs, will only process any newly added models.\n",
    "If execution is interrupted, you can simply rerun and it will resume from the last saved state.\n",
    "\"\"\"\n",
    "\n",
    "# path of the JSON file where results are stored\n",
    "json_path = os.path.join(output_dir, \"scaling_results.json\")\n",
    "\n",
    "if os.path.exists(study_path):\n",
    "    # batch size (speeds up the process at bit)\n",
    "    batch_size = 128 # adjust this according to your GPU memory\n",
    "    # path to the validation dataset\n",
    "    eval_path = \"../graph/val.pt\"\n",
    "    # load the data on which we want to evaluate the models\n",
    "    eval_data = load_torch_data(eval_path)\n",
    "    dataloader = create_dataloader(\n",
    "        eval_data, \n",
    "        num_data=-1, # use the whole dataset \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, # do not shuffle the validation set\n",
    "    )\n",
    "    print(f\"Loaded evaluation data from '{eval_path:s}'\", flush=True)\n",
    "    # load or initialize results\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"Loaded existing results ({len(extract_processed_dirs(results)):d} runs)\")\n",
    "    else:\n",
    "        results = init_empty_results()\n",
    "        print(\"Initialized new results store\")\n",
    "    # find what hass already been done\n",
    "    processed = extract_processed_dirs(results)\n",
    "    # scan for all study subdirectories\n",
    "    all_dirs = sorted(d for d in os.listdir(study_path) if os.path.isdir(os.path.join(study_path, d)))\n",
    "    # only process the new ones\n",
    "    new_dirs = [d for d in all_dirs if d not in processed]\n",
    "    save_every = 4  \n",
    "    total = len(new_dirs)\n",
    "    if total == 0:\n",
    "        print(\"No new studies to process, everything is up to date\")\n",
    "    else:\n",
    "        print(f\"Processing {len(new_dirs)} new studies (saving every {save_every:d})\")\n",
    "        for idx, study_dir in enumerate(new_dirs, start=1):\n",
    "            process_one(\n",
    "                study_path=study_path,\n",
    "                study_dir=study_dir, \n",
    "                results=results, \n",
    "                dataloader=dataloader,\n",
    "            )\n",
    "            # checkpoint every 'save_every' or on the very last one\n",
    "            if (idx % save_every == 0) or (idx == total):\n",
    "                with open(json_path, \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                print(f\"    Checkpointed after {idx:d}/{total:d} runs\")\n",
    "        print(f\"All {total:d} new runs appended and final JSON saved\")\n",
    "else:\n",
    "    with open(json_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"Loaded results for {len(extract_processed_dirs(results)):d} models from JSON file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling law plots: Single Power Law Tests (Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with or without errorbars\n",
    "error_bars = True\n",
    "\n",
    "# helper variables for aicc table\n",
    "scaling_law_labels = {\n",
    "    \"simple\": r\"Power Law\",\n",
    "    \"floor\": r\"Power Law + Floor\",\n",
    "    \"broken\": r\"Broken Power Law\",\n",
    "    \"broken_with_amp\": r\"Broken power law + Amplitude\",\n",
    "}\n",
    "model_order = [\"OptiMetal2B (CGC)\", \"OptiMetal2B (TC)\", \"OptiMetal3B (TC)\"]\n",
    "\n",
    "# loop over all metrics and data scaling function types \n",
    "for metric_key in [\"val_loss\"]:\n",
    "    \n",
    "    # aicc table setup\n",
    "    df = pd.DataFrame(\n",
    "        index=[scaling_law_labels[k] for k in [\"simple\",\"floor\",\"broken\",\"broken_with_amp\"]],\n",
    "        columns=model_order, \n",
    "        dtype=float,\n",
    "    )\n",
    "    \n",
    "    for func_type in [\"simple\", \"floor\", \"broken\", \"broken_with_amp\"]:\n",
    "        \n",
    "        # aicc table setup\n",
    "        aicc_row = {m: np.nan for m in model_order}\n",
    "        row_label = scaling_law_labels[func_type]\n",
    "        \n",
    "        # figure setup\n",
    "        fig, ax = plt.subplots(figsize=(3.5, 3.0))\n",
    "\n",
    "        # parameters and useful variable\n",
    "        var = \"D\"\n",
    "        func = get_power_law(func_type)\n",
    "        fits = [] # to store (label, alpha, const, color) for the legend\n",
    "        fit_params = []\n",
    "        fname = f\"{metric_key:s}_{func_type:s}_scaling_laws_data\"\n",
    "        \n",
    "        # some fits can be unstable...\n",
    "        try:\n",
    "\n",
    "            # plot setup\n",
    "            num_data = sorted([int(x) for x in results[\"data\"][\"2b\"][\"variant1\"].keys()]) # tick positions for the x-axis\n",
    "            x_fit = np.logspace(np.log10(2000), np.log10(200000), 100)\n",
    "\n",
    "            # OptiMetal2B CGC\n",
    "            color = \"k\"\n",
    "            name = \"OptiMetal2B (CGC)\"\n",
    "            data = results[\"data\"][\"2b\"][\"variant1\"]\n",
    "            plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars)\n",
    "            fit_dict = fit_scaling_law(data, func_type=func_type, key=metric_key)\n",
    "            y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "            ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "            fits.append([name, fit_dict[\"popt\"], color])\n",
    "            fit_params.append([name, *list(fit_dict.values())])\n",
    "            aicc_row[name] = np.round((fit_dict[\"aicc\"]), 2)\n",
    "\n",
    "            # OptiMetal2B CGC\n",
    "            color = \"tab:orange\"\n",
    "            name = \"OptiMetal2B (TC)\"\n",
    "            data = results[\"data\"][\"2b\"][\"variant2\"]\n",
    "            plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars)\n",
    "            fit_dict = fit_scaling_law(data, func_type=func_type, key=metric_key)\n",
    "            y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "            ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "            fits.append([name, fit_dict[\"popt\"], color])\n",
    "            fit_params.append([name, *list(fit_dict.values())])\n",
    "            aicc_row[name] = np.round((fit_dict[\"aicc\"]), 2)\n",
    "\n",
    "            # OptiMetal3B\n",
    "            color = \"tab:blue\"\n",
    "            name = \"OptiMetal3B (TC)\"\n",
    "            data = results[\"data\"][\"3b\"]\n",
    "            plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars)\n",
    "            fit_dict = fit_scaling_law(data, func_type=func_type, key=metric_key)\n",
    "            y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "            ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "            fits.append([name, fit_dict[\"popt\"], color])\n",
    "            fit_params.append([name, *list(fit_dict.values())])\n",
    "            aicc_row[name] = np.round((fit_dict[\"aicc\"]), 2)\n",
    "                \n",
    "            # rows for the latex table\n",
    "            rows = build_rows_from_fits(fit_params, func_type=func_type, X=var)\n",
    "\n",
    "            # log-log plot and axis ticks\n",
    "            yticks = ax.get_yticks()\n",
    "            ax.set_xlim(2000, 200000)\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xticks(num_data)\n",
    "            ax.xaxis.set_major_locator(FixedLocator(num_data))\n",
    "            ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "            ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(int(x))))\n",
    "            ax.tick_params(axis=\"x\", which=\"minor\", length=0)\n",
    "            ax.yaxis.set_minor_locator(FixedLocator(yticks))\n",
    "            ax.yaxis.set_major_locator(FixedLocator(yticks))\n",
    "            ax.yaxis.set_minor_formatter(ScalarFormatter())\n",
    "            ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "            ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "\n",
    "            # axis labels and legends\n",
    "            ax.set_xlabel(r\"$D$\")\n",
    "            ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "            leg_models = ax.legend(loc=\"lower left\", handletextpad=0.25, handlelength=1.25)\n",
    "            fit_handle = [Line2D([], [], ls=\"--\", color=\"0.5\")]\n",
    "            fit_label = [get_fit_func_str(func_type, var) for _ in fits]\n",
    "            ax.legend(fit_handle, fit_label, title=r\"$N \\approx 10\\mathrm{M}$ ($d_\\mathrm{h}=256$)\", loc=\"upper right\", handletextpad=0.25, handlelength=1.25)\n",
    "            ax.add_artist(leg_models)\n",
    "\n",
    "            # save the figure\n",
    "            fig.tight_layout()\n",
    "            fig.align_labels()\n",
    "            fig.savefig(os.path.join(fig_dir, fname + \".pdf\"))\n",
    "            fig.savefig(os.path.join(fig_dir, fname + \".svg\"))\n",
    "\n",
    "            # print and save the latex tables\n",
    "            latex = latex_table_from_rows(rows, X=\"D\", fit_func_display=False)\n",
    "            print(f\"\\nData scaling: {metric_key:s} & {func_type:s}\")\n",
    "            print(latex)\n",
    "            with open(os.path.join(fig_dir, fname + \".txt\"), \"w\") as f:\n",
    "                f.write(latex)\n",
    "                \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"[WARN] {fname:s}: An error occurred, probably due to an unstable fit ({e})\\n\")\n",
    "            with open(os.path.join(fig_dir, fname + \".txt\"), \"w\") as f:\n",
    "                f.write(\"Unstable fit...\")\n",
    "                \n",
    "        finally:\n",
    "            plt.close(fig)        \n",
    "                \n",
    "        # aicc table dataframe\n",
    "        df.loc[row_label, model_order] = [aicc_row[m] for m in model_order]\n",
    "\n",
    "    # latex table to compare the aicc of different scaling fit functions\n",
    "    latex_aicc = df.to_latex(\n",
    "        index=True,\n",
    "        escape=False,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format=\"c\",\n",
    "        bold_rows=False,\n",
    "        float_format=lambda x: f\"{int(round(x))}\" if np.isclose(x, round(x)) else f\"{x:.2f}\",\n",
    "        column_format=\"l\" + len(df.columns) * r\"c@{\\hspace{1em}}\",\n",
    "    )\n",
    "    print(\"\\nAICc table:\")\n",
    "    print(latex_aicc)\n",
    "    with open(os.path.join(fig_dir, f\"{metric_key:s}_aicc_table_data.txt\"), \"w\") as f:\n",
    "        f.write(latex_aicc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling law plots: Single Power Law Tests (Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with or without errorbars\n",
    "error_bars = True\n",
    "\n",
    "# helper variables for aicc table\n",
    "scaling_law_labels = {\n",
    "    \"simple\": r\"Power Law\",\n",
    "    \"floor\": r\"Power Law + Floor\",\n",
    "    \"broken\": r\"Broken Power Law\",\n",
    "    \"broken_with_amp\": r\"Broken power law + Amplitude\",\n",
    "}\n",
    "model_order = [\"OptiMetal2B (CGC)\", \"OptiMetal2B (TC)\", \"OptiMetal3B (TC)\"]\n",
    "\n",
    "# loop over all metrics\n",
    "for metric_key in [\"val_loss\"]:\n",
    "    \n",
    "    # aicc table setup\n",
    "    df = pd.DataFrame(\n",
    "        index=[scaling_law_labels[k] for k in [\"simple\",\"floor\",\"broken\",\"broken_with_amp\"]],\n",
    "        columns=model_order, \n",
    "        dtype=float,\n",
    "    )\n",
    "    \n",
    "    for func_type in [\"simple\", \"floor\", \"broken\", \"broken_with_amp\"]:\n",
    "        \n",
    "        # aicc table setup\n",
    "        aicc_row = {m: np.nan for m in model_order}\n",
    "        row_label = scaling_law_labels[func_type]\n",
    "        \n",
    "        # figure setup\n",
    "        fig, ax = plt.subplots(figsize=(3.5, 3.0))\n",
    "\n",
    "        # parameters and useful variable\n",
    "        var = \"N\"\n",
    "        func = get_power_law(func_type)\n",
    "        fits = [] # to store (label, alpha, const, color) for the legend\n",
    "        fit_params = []\n",
    "        fname = f\"{metric_key:s}_{func_type:s}_scaling_laws_parameter\"\n",
    "        \n",
    "        # some fits may be unstable...\n",
    "        try:\n",
    "\n",
    "            # plot setup\n",
    "            num_parameter = [5e5, 1e6, 5e6, 1e7, 5e7, 1e8] # tick positions for the x-axis\n",
    "            x_fit = np.logspace(np.log10(2e5), np.log10(2e8), 100)\n",
    "\n",
    "            # OptiMetal2B CGC\n",
    "            color = \"k\"\n",
    "            name = \"OptiMetal2B (CGC)\"\n",
    "            data = results[\"parameter\"][\"2b\"][\"variant1\"]\n",
    "            plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars, x_from_entry=\"num_parameter\")\n",
    "            fit_dict = fit_scaling_law(data, func_type=func_type, key=metric_key, x_from_entry=\"num_parameter\")\n",
    "            y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "            ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "            fits.append([name, fit_dict[\"popt\"], color])\n",
    "            fit_params.append([name, *list(fit_dict.values())])\n",
    "            aicc_row[name] = np.round((fit_dict[\"aicc\"]), 2)\n",
    "\n",
    "            # OptiMetal2B CGC\n",
    "            color = \"tab:orange\"\n",
    "            name = \"OptiMetal2B (TC)\"\n",
    "            data = results[\"parameter\"][\"2b\"][\"variant2\"]\n",
    "            plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars, x_from_entry=\"num_parameter\")\n",
    "            fit_dict = fit_scaling_law(data, func_type=func_type, key=metric_key, x_from_entry=\"num_parameter\")\n",
    "            y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "            ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "            fits.append([name, fit_dict[\"popt\"], color])\n",
    "            fit_params.append([name, *list(fit_dict.values())])\n",
    "            aicc_row[name] = np.round((fit_dict[\"aicc\"]), 2)\n",
    "\n",
    "            # OptiMetal3B\n",
    "            color = \"tab:blue\"\n",
    "            name = \"OptiMetal3B (TC)\"\n",
    "            data = results[\"parameter\"][\"3b\"]\n",
    "            plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars, x_from_entry=\"num_parameter\")\n",
    "            fit_dict = fit_scaling_law(data, func_type=func_type, key=metric_key, x_from_entry=\"num_parameter\")\n",
    "            y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "            ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "            fits.append([name, fit_dict[\"popt\"], color])\n",
    "            fit_params.append([name, *list(fit_dict.values())])\n",
    "            aicc_row[name] = np.round((fit_dict[\"aicc\"]), 2)\n",
    "                \n",
    "            # rows for the latex table\n",
    "            rows = build_rows_from_fits(fit_params, func_type=func_type, X=var)\n",
    "\n",
    "            # log-log plot and axis ticks\n",
    "            yticks = ax.get_yticks()\n",
    "            ax.set_xlim(2e5, 2e8)\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xticks(num_parameter)\n",
    "            ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(int(x))))\n",
    "            ax.yaxis.set_minor_locator(FixedLocator(yticks))\n",
    "            ax.yaxis.set_major_locator(FixedLocator(yticks))\n",
    "            ax.yaxis.set_minor_formatter(ScalarFormatter())\n",
    "            ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "            ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "\n",
    "            # axis labels and legends\n",
    "            ax.set_xlabel(r\"$N$\")\n",
    "            ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "            model_handles, model_labels = ax.get_legend_handles_labels()\n",
    "            fit_handle = Line2D([], [], ls=\"--\", color=\"0.5\")\n",
    "            fit_label = get_fit_func_str(func_type, var)\n",
    "            handles = [fit_handle] + model_handles\n",
    "            labels =  [fit_label] + model_labels\n",
    "            ax.legend(handles, labels, title=r\"$D=20000$\", loc=\"upper right\", handletextpad=0.25, handlelength=1.25)\n",
    "\n",
    "            # save the figure\n",
    "            fig.tight_layout()\n",
    "            fig.align_labels()\n",
    "            fig.savefig(os.path.join(fig_dir, fname + \".pdf\"))\n",
    "            fig.savefig(os.path.join(fig_dir, fname + \".svg\"))\n",
    "\n",
    "            # print and save the latex tables\n",
    "            latex = latex_table_from_rows(rows, X=\"N\", fit_func_display=False)\n",
    "            print(f\"\\nParameter scaling: {metric_key:s} & {func_type:s}\")\n",
    "            print(latex)\n",
    "            with open(os.path.join(fig_dir, fname + \".txt\"), \"w\") as f:\n",
    "                f.write(latex)\n",
    "                \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"[WARN] {fname:s}: An error occurred, probably due to an unstable fit ({e})\\n\")\n",
    "            with open(os.path.join(fig_dir, fname + \".txt\"), \"w\") as f:\n",
    "                f.write(\"Unstable fit...\")\n",
    "                \n",
    "        finally:\n",
    "            plt.close(fig)        \n",
    "                \n",
    "        # aicc table dataframe\n",
    "        df.loc[row_label, model_order] = [aicc_row[m] for m in model_order]\n",
    "\n",
    "    # latex table to compare the aicc of different scaling fit functions\n",
    "    latex_aicc = df.to_latex(\n",
    "        index=True,\n",
    "        escape=False,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format=\"c\",\n",
    "        bold_rows=False,\n",
    "        float_format=lambda x: f\"{int(round(x))}\" if np.isclose(x, round(x)) else f\"{x:.2f}\",\n",
    "        column_format=\"l\" + len(df.columns) * r\"c@{\\hspace{1em}}\",\n",
    "    )\n",
    "    print(\"\\nAICc table:\")\n",
    "    print(latex_aicc)\n",
    "    with open(os.path.join(fig_dir, f\"{metric_key:s}_aicc_table_parameter.txt\"), \"w\") as f:\n",
    "        f.write(latex_aicc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling law plots: Single Power Laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with or without errorbars\n",
    "error_bars = True\n",
    "\n",
    "# helper variables for aicc table\n",
    "model_order = [\"OptiMetal2B (CGC)\", \"OptiMetal2B (TC)\", \"OptiMetal3B (TC)\"]\n",
    "\n",
    "# loop over all metrics and function types \n",
    "for metric_key in [\"val_loss\"]:\n",
    "\n",
    "    # figure setup\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(3.5, 5))\n",
    "    fname = f\"{metric_key:s}_scaling_laws\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Data scaling\n",
    "    \"\"\"\n",
    "\n",
    "    # parameters and useful variable\n",
    "    var = \"D\"\n",
    "    data_func_type = \"broken\"\n",
    "    func = get_power_law(data_func_type)\n",
    "    fits = [] # to store (label, alpha, const, color) for the legend\n",
    "    fit_params_data = []\n",
    "    \n",
    "    # some fits may be unstable...\n",
    "    try:\n",
    "\n",
    "        # plot setup\n",
    "        ax = axes[0]\n",
    "        num_data = sorted([int(x) for x in results[\"data\"][\"2b\"][\"variant1\"].keys()]) # tick positions for the x-axis\n",
    "        x_fit = np.logspace(np.log10(2000), np.log10(200000), 100)\n",
    "\n",
    "        # OptiMetal2B CGC\n",
    "        color = \"k\"\n",
    "        name = \"OptiMetal2B (CGC)\"\n",
    "        data = results[\"data\"][\"2b\"][\"variant1\"]\n",
    "        plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars)\n",
    "        fit_dict = fit_scaling_law(data, func_type=data_func_type, key=metric_key)\n",
    "        y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "        ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "        fits.append([name, fit_dict[\"popt\"], color])\n",
    "        fit_params_data.append([name, *list(fit_dict.values())])\n",
    "        \n",
    "        # add \"unbroken\" power-law fit for comparison\n",
    "        data = _filter_nonempty(data)\n",
    "        if len(data) < 2:\n",
    "            raise ValueError(\"Need at least two points for a fit\")\n",
    "        sort_idx = np.argsort([int(x) for x in data.keys()])\n",
    "        x = np.array(sorted([int(x) for x in data.keys()]), dtype=float)\n",
    "        y_mean = np.array(\n",
    "            [np.mean([entry[metric_key] for entry in data[k]]) for k in np.array(list(data.keys()))[sort_idx]], \n",
    "            dtype=float,\n",
    "        )\n",
    "        y_std = np.array(\n",
    "            [np.std([entry[metric_key] for entry in data[k]]) for k in np.array(list(data.keys()))[sort_idx]], \n",
    "            dtype=float,\n",
    "        )\n",
    "        alpha_1, d0_1 = np.polyfit(np.log10(x[:3]), np.log10(y_mean[:3]), deg=1)\n",
    "        alpha_2, d0_2 = np.polyfit(np.log10(x[-3:]), np.log10(y_mean[-3:]), deg=1)\n",
    "        y_fit_unbroken_1 = 10**d0_1 * x_fit**alpha_1\n",
    "        ax.plot(x_fit, y_fit_unbroken_1, \":\", color=\"k\", zorder=-1)\n",
    "        y_fit_unbroken_2 = 10**d0_2 * x_fit**alpha_2\n",
    "        ax.plot(x_fit, y_fit_unbroken_2, \":\", color=\"k\", zorder=-1, label=\"Asymptotic fit\")\n",
    "\n",
    "        # OptiMetal2B CGC\n",
    "        color = \"tab:orange\"\n",
    "        name = \"OptiMetal2B (TC)\"\n",
    "        data = results[\"data\"][\"2b\"][\"variant2\"]\n",
    "        plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars)\n",
    "        fit_dict = fit_scaling_law(data, func_type=data_func_type, key=metric_key)\n",
    "        y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "        ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "        fits.append([name, fit_dict[\"popt\"], color])\n",
    "        fit_params_data.append([name, *list(fit_dict.values())])\n",
    "\n",
    "        # OptiMetal3B\n",
    "        color = \"tab:blue\"\n",
    "        name = \"OptiMetal3B (TC)\"\n",
    "        data = results[\"data\"][\"3b\"]\n",
    "        plot_scaling_law(ax, data, name, key=metric_key, color=color, error_bars=error_bars)\n",
    "        fit_dict = fit_scaling_law(data, func_type=data_func_type, key=metric_key)\n",
    "        y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "        ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "        fits.append([name, fit_dict[\"popt\"], color])\n",
    "        fit_params_data.append([name, *list(fit_dict.values())])\n",
    "            \n",
    "        # rows for the latex table\n",
    "        rows_data = build_rows_from_fits(fit_params_data, func_type=data_func_type, X=var)\n",
    "\n",
    "        # log-log plot and axis ticks\n",
    "        yticks = ax.get_yticks()\n",
    "        ax.set_xlim(2000, 200000)\n",
    "        ax.set_ylim(top=2.5)\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xticks(num_data)\n",
    "        ax.xaxis.set_major_locator(FixedLocator(num_data))\n",
    "        ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(int(x))))\n",
    "        ax.tick_params(axis=\"x\", which=\"minor\", length=0)\n",
    "        ax.yaxis.set_minor_locator(FixedLocator(yticks))\n",
    "        ax.yaxis.set_major_locator(FixedLocator(yticks))\n",
    "        ax.yaxis.set_minor_formatter(ScalarFormatter())\n",
    "        ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "        ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "\n",
    "        # axis labels and legends\n",
    "        ax.set_xlabel(r\"$D$\")\n",
    "        ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "        leg_models = ax.legend(loc=\"lower left\", handletextpad=0.25, handlelength=1.25)\n",
    "        fit_handle = [Line2D([], [], ls=\"--\", color=\"tab:gray\")]\n",
    "        fit_label = [get_fit_func_str(data_func_type, var)]\n",
    "        leg = ax.legend(fit_handle, fit_label, title=r\"$N \\approx 10\\mathrm{M}$ ($d_\\mathrm{h}=256$)\", loc=\"upper right\", handletextpad=0.25, handlelength=1.25)\n",
    "        ax.add_artist(leg_models)\n",
    "\n",
    "        \"\"\"\n",
    "        Parameter scaling\n",
    "        \"\"\"\n",
    "\n",
    "        # parameters and useful variable\n",
    "        var = \"N\"\n",
    "        param_func_type = \"floor\"\n",
    "        func = get_power_law(param_func_type)\n",
    "        fit_params_parameter = []\n",
    "\n",
    "        # plot setup\n",
    "        ax = axes[1]\n",
    "        num_parameter = [5e5, 1e6, 5e6, 1e7, 5e7, 1e8] # tick positions for the x-axis\n",
    "        x_fit = np.logspace(np.log10(2e5), np.log10(2e8), 100)\n",
    "\n",
    "        # OptiMetal2B CGC\n",
    "        color = \"k\"\n",
    "        name = \"OptiMetal2B (CGC)\"\n",
    "        data = results[\"parameter\"][\"2b\"][\"variant1\"]\n",
    "        plot_scaling_law(ax, data, None, key=metric_key, color=color, error_bars=error_bars, x_from_entry=\"num_parameter\")\n",
    "        fit_dict = fit_scaling_law(data, func_type=param_func_type, key=metric_key, x_from_entry=\"num_parameter\")\n",
    "        y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "        ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1, label=get_fit_func_str(param_func_type, var))\n",
    "        fit_params_parameter.append([name, *list(fit_dict.values())])\n",
    "\n",
    "        # OptiMetal2B CGC\n",
    "        color = \"tab:orange\"\n",
    "        name = \"OptiMetal2B (TC)\"\n",
    "        data = results[\"parameter\"][\"2b\"][\"variant2\"]\n",
    "        plot_scaling_law(ax, data, None, key=metric_key, color=color, error_bars=error_bars, x_from_entry=\"num_parameter\")\n",
    "        fit_dict = fit_scaling_law(data, func_type=param_func_type, key=metric_key, x_from_entry=\"num_parameter\")\n",
    "        y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "        ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "        fit_params_parameter.append([name, *list(fit_dict.values())])\n",
    "\n",
    "        # OptiMetal3B\n",
    "        color = \"tab:blue\"\n",
    "        name = \"OptiMetal3B (TC)\"\n",
    "        data = results[\"parameter\"][\"3b\"]\n",
    "        plot_scaling_law(ax, data, None, key=metric_key, color=color, error_bars=error_bars, x_from_entry=\"num_parameter\")\n",
    "        fit_dict = fit_scaling_law(data, func_type=param_func_type, key=metric_key, x_from_entry=\"num_parameter\")\n",
    "        y_fit = func(x_fit, *fit_dict[\"popt\"])\n",
    "        ax.plot(x_fit, y_fit, \"--\", color=color, zorder=-1)\n",
    "        fit_params_parameter.append([name, *list(fit_dict.values())])\n",
    "\n",
    "        # rows for the latex table\n",
    "        rows_params = build_rows_from_fits(fit_params_parameter, func_type=param_func_type, X=var)\n",
    "\n",
    "        # log-log plot and axis ticks\n",
    "        yticks = ax.get_yticks()\n",
    "        ax.set_xlim(2e5, 2e8)\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xticks(num_parameter)\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(int(x))))\n",
    "        ax.yaxis.set_minor_locator(FixedLocator(yticks))\n",
    "        ax.yaxis.set_major_locator(FixedLocator(yticks))\n",
    "        ax.yaxis.set_minor_formatter(ScalarFormatter())\n",
    "        ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "        ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "\n",
    "        # axis labels and legends\n",
    "        ax.set_xlabel(r\"$N$\")\n",
    "        ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "        fit_handle = [Line2D([], [], ls=\"--\", color=\"tab:gray\")]\n",
    "        fit_label = [get_fit_func_str(param_func_type, var)]\n",
    "        leg = ax.legend(fit_handle, fit_label, title=r\"$D=20000$\", loc=\"upper right\", handletextpad=0.25, handlelength=1.25)\n",
    "\n",
    "        # save the figure\n",
    "        fig.tight_layout()\n",
    "        fig.align_labels()\n",
    "        fig.savefig(os.path.join(fig_dir, fname + \".pdf\"))\n",
    "        fig.savefig(os.path.join(fig_dir, fname + \".svg\"))\n",
    "\n",
    "        # print and save the latex tables\n",
    "        latex_data = latex_table_from_rows(rows_data, X=\"D\", fit_func_display=False)\n",
    "        latex_params = latex_table_from_rows(rows_params, X=\"N\", fit_func_display=False)\n",
    "        print(f\"\\nData scaling:\")\n",
    "        print(latex_data)\n",
    "        print(f\"\\nParameter scaling:\")\n",
    "        print(latex_data)\n",
    "        with open(os.path.join(fig_dir, fname + \".txt\"), \"w\") as f:\n",
    "            f.write(latex_data)\n",
    "            f.write(\"\\n\")\n",
    "            f.write(latex_params)\n",
    "            \n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"[WARN] {fname:s}: An error occurred, probably due to an unstable fit ({e})\\n\")\n",
    "        with open(os.path.join(fig_dir, fname + \".txt\"), \"w\") as f:\n",
    "            f.write(\"Unstable fit...\"),\n",
    "            \n",
    "    finally:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the results from the \"grid search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing the scaling law study\n",
    "study_path = \"/scratch/magr4985/Scaling_Grid\"\n",
    "\n",
    "# directory to save the results\n",
    "output_dir = \"./scaling_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# figure directory\n",
    "fig_dir = \"./scaling_data/scaling_grid\"\n",
    "os.makedirs(fig_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the all training runs converged\n",
    "if os.path.exists(study_path):\n",
    "    # load all study directories, valdation loss files, and tensorboard logs\n",
    "    study_dirs = [d for d in os.listdir(study_path) if os.path.isdir(os.path.join(study_path, d))]\n",
    "    print(f\"[INFO] Found {len(study_dirs):d} study directories in {study_path:s}\")\n",
    "    checks = []\n",
    "    for study_dir in study_dirs:\n",
    "        val_loss = None\n",
    "        if os.path.exists(os.path.join(study_path, study_dir, \"val_loss.txt\")):\n",
    "            val_loss = float(np.loadtxt(os.path.join(study_path, study_dir, \"val_loss.txt\")))\n",
    "        checks.append([study_dir, val_loss, load_tb_scalars(os.path.join(study_path, study_dir))])\n",
    "    # print the study directories with diverging gradients, i.e, where something went wrong\n",
    "    print(\"\\n[DIVERGING GRADIENTS CHECK]\")\n",
    "    for study_dir, val_loss, tb_log in checks:\n",
    "        if np.max(tb_log[\"train/grad_norm\"]) > 1e2:\n",
    "            print(\"[DIVERGING GRADIENTS] \", study_dir, np.max(tb_log[\"train/grad_norm\"]))\n",
    "    # print study directories with training not finished\n",
    "    print(\"\\n[TRAINING FINISHED CHECK]\")\n",
    "    for study_dir, val_loss, tb_log in checks:\n",
    "        if len(tb_log.get(\"val/loss\", [])) < 500:\n",
    "            print(\"[TRAINING NOT FINISHED] \", study_dir, len(tb_log.get(\"val/loss\", [])))\n",
    "    # seed-to-seed validation-loss consistency\n",
    "    print(\"\\n[SEED VARIANCE CHECK]\")\n",
    "    max_seed_variance_thr = 0.05\n",
    "    _seed_suffix_re = re.compile(r'([_-])seed\\d+$')\n",
    "    _seed_extract_re = re.compile(r'seed(\\d+)$')\n",
    "    def _key_without_seed(name: str) -> str:\n",
    "        return _seed_suffix_re.sub('', name)\n",
    "    def _seed_or_inf(name: str) -> int:\n",
    "        m = _seed_extract_re.search(name)\n",
    "        return int(m.group(1)) if m else 10**9 # for nice sorting when seed missing\n",
    "    groups = defaultdict(list)\n",
    "    for study_dir, val_loss, _ in checks:\n",
    "        if val_loss is not None and np.isfinite(val_loss):\n",
    "            groups[_key_without_seed(study_dir)].append((study_dir, float(val_loss)))\n",
    "    for cfg_key, items in groups.items():\n",
    "        if len(items) < 2:\n",
    "            continue # need at least two seeds to compare\n",
    "        vals = [v for _, v in items]\n",
    "        sorted_vals = sorted(vals)\n",
    "        low = sorted_vals[0]\n",
    "        mid = sorted_vals[len(sorted_vals) // 2]\n",
    "        high = sorted_vals[-1]\n",
    "        if low <= 0:\n",
    "            continue # avoid divide-by-zero/undefined relative difference\n",
    "        max_rel_spread = max((mid - low) / mid, (high - mid) / mid)\n",
    "        if max_rel_spread > max_seed_variance_thr:\n",
    "            print(f\"[SEED VARIANCE > 5%] {cfg_key:s}: max(spread)={100*max_rel_spread:.2f}% (low={low:.6g}, mid={mid:.6g} , high={high:.6g})\")\n",
    "            for sd, v in sorted(items, key=lambda p: _seed_or_inf(p[0])):\n",
    "                print(f\"    {sd:s}: val_loss={v:.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incrementally load and evaluate all models in the scaling-law study, caching results to disk.\n",
    "\n",
    "Evaluating every model can be time-consuming, so this process saves intermediate results\n",
    "to a JSON file and, on subsequent runs, will only process any newly added models.\n",
    "If execution is interrupted, you can simply rerun and it will resume from the last saved state.\n",
    "\"\"\"\n",
    "\n",
    "# path of the JSON file where results are stored\n",
    "json_path = os.path.join(output_dir, \"scaling_grid_results.json\")\n",
    "\n",
    "if os.path.exists(study_path):\n",
    "    # batch size (speeds up the process at bit)\n",
    "    batch_size = 128 # adjust this according to your GPU memory\n",
    "    # path to the validation dataset\n",
    "    eval_path = \"../graph/val.pt\"\n",
    "    # load the data on which we want to evaluate the models\n",
    "    eval_data = load_torch_data(eval_path)\n",
    "    dataloader = create_dataloader(\n",
    "        eval_data, \n",
    "        num_data=-1, # use the whole dataset \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, # do not shuffle the validation set\n",
    "    )\n",
    "    print(f\"Loaded evaluation data from '{eval_path:s}'\", flush=True)\n",
    "    # load or initialize results\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"Loaded existing results ({len(extract_processed_dirs(results)):d} runs)\")\n",
    "    else:\n",
    "        results = init_empty_results_grid()\n",
    "        print(\"Initialized new results store\")\n",
    "    # find what hass already been done\n",
    "    processed = extract_processed_dirs(results)\n",
    "    # scan for all study subdirectories\n",
    "    all_dirs = sorted(d for d in os.listdir(study_path) if os.path.isdir(os.path.join(study_path, d)))\n",
    "    # only process the new ones\n",
    "    new_dirs = [d for d in all_dirs if d not in processed]\n",
    "    save_every = 4  \n",
    "    total = len(new_dirs)\n",
    "    if total == 0:\n",
    "        print(\"No new studies to process, everything is up to date\")\n",
    "    else:\n",
    "        print(f\"Processing {len(new_dirs)} new studies (saving every {save_every:d})\")\n",
    "        for idx, study_dir in enumerate(new_dirs, start=1):\n",
    "            process_one_grid(\n",
    "                study_path=study_path,\n",
    "                study_dir=study_dir, \n",
    "                results=results, \n",
    "                dataloader=dataloader,\n",
    "            )\n",
    "            # checkpoint every 'save_every' or on the very last one\n",
    "            if (idx % save_every == 0) or (idx == total):\n",
    "                with open(json_path, \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                print(f\"    Checkpointed after {idx:d}/{total:d} runs\")\n",
    "        print(f\"All {total:d} new runs appended and final JSON saved\")\n",
    "else:\n",
    "    with open(json_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"Loaded results for {len(extract_processed_dirs(results)):d} models from JSON file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling law grid maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color map name\n",
    "cmap_name = \"viridis_r\"\n",
    "cmap = plt.get_cmap(cmap_name)\n",
    "\n",
    "# plot with or without errorbars\n",
    "error_bars = True\n",
    "\n",
    "# unified validation loss axis limits\n",
    "lmin = 0.40\n",
    "lmax = 1.85\n",
    "\n",
    "# helper function for latex table\n",
    "model_label_helper = lambda key: {\"2b\": \"OptiMetal2B (TC)\", \"3b\": \"OptiMetal3B (TC)\"}.get(key, str(key))\n",
    "\n",
    "# loop over all metrics \n",
    "for metric_key in [\"val_loss\"]:\n",
    "    \n",
    "    # loop over all fit types\n",
    "    for fit_type in [\"kaplan\", \"hoffmann\"]:\n",
    "        \n",
    "        # latex table row setup\n",
    "        param_rows = []\n",
    "\n",
    "        # loop over all models\n",
    "        for model in results.keys():\n",
    "            \n",
    "            # process the data into a more manageable format\n",
    "            datapoints, widths, num_parameters, mus, stds = grid_to_vectors(results[model], metric_key=metric_key)\n",
    "\n",
    "            # fit the loss surface and store the results to create a latex table later one\n",
    "            X = (datapoints.ravel(), num_parameters.ravel())\n",
    "            if fit_type == \"kaplan\":\n",
    "                func = global_scaling_fit_kaplan\n",
    "                popt, pcov = curve_fit(\n",
    "                    f=func,\n",
    "                    xdata=X,\n",
    "                    ydata=mus.ravel(),\n",
    "                    p0=[0.5, 0.5, 0.5, 1e4, 1e4],\n",
    "                    sigma=stds.ravel(),\n",
    "                    absolute_sigma=True,\n",
    "                    maxfev=100000,\n",
    "                )\n",
    "            elif fit_type == \"hoffmann\":\n",
    "                func = global_scaling_fit_hoffmann\n",
    "                popt, pcov = curve_fit(\n",
    "                    f=func,\n",
    "                    xdata=X,\n",
    "                    ydata=mus.ravel(),\n",
    "                    p0=[0.5, 0.5, 0.5, 1e4, 1e4, 0],\n",
    "                    sigma=stds.ravel(),\n",
    "                    absolute_sigma=True,\n",
    "                    maxfev=100000,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported fit type\")\n",
    "            perr = np.sqrt(np.diag(pcov))\n",
    "            y_fit = func(X, *popt)\n",
    "            res = mus.ravel() - y_fit\n",
    "            aicc = calc_aicc(res, len(popt))\n",
    "            if fit_type == \"kaplan\":\n",
    "                row = {\n",
    "                    \"Model\": model_label_helper(model),\n",
    "                    r\"$\\alpha_N$\": format_val_err(popt[0], perr[0]),\n",
    "                    r\"$\\alpha_{D,1}$\": format_val_err(popt[1], perr[1]),\n",
    "                    r\"$\\alpha_{D,2}$\": format_val_err(popt[2], perr[2]),\n",
    "                    r\"$N_c$\": format_val_err_log10(popt[3], perr[3]),\n",
    "                    r\"$D_c$\": format_val_err_log10(popt[4], perr[4]),\n",
    "                    r\"AICc\": f\"{aicc:.2f}\",\n",
    "                }\n",
    "            elif fit_type == \"hoffmann\":\n",
    "                row = {\n",
    "                    \"Model\": model_label_helper(model),\n",
    "                    r\"$\\alpha_N$\": format_val_err(popt[0], perr[0]),\n",
    "                    r\"$\\alpha_{D,1}$\": format_val_err(popt[1], perr[1]),\n",
    "                    r\"$\\alpha_{D,2}$\": format_val_err(popt[2], perr[2]),\n",
    "                    r\"$N_c$\": format_val_err_log10(popt[3], perr[3]),\n",
    "                    r\"$D_c$\": format_val_err_log10(popt[4], perr[4]),\n",
    "                    r\"L_0\": fmt_param(popt[5], perr[5]),\n",
    "                    r\"AICc\": f\"{aicc:.2f}\",\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported fit type\")\n",
    "            param_rows.append(row)\n",
    "\n",
    "            # setup the figure\n",
    "            fig = plt.figure(figsize=(3.5, 6), constrained_layout=True)\n",
    "            gs = fig.add_gridspec(3, 2, width_ratios=[1.0, 0.1])\n",
    "            \n",
    "            # plot the valdation loss over D for every N\n",
    "            ax = fig.add_subplot(gs[1, 0]) \n",
    "            norm_n = LogNorm(vmin=np.min(num_parameters), vmax=np.max(num_parameters))\n",
    "            for i in range(datapoints.shape[1]):\n",
    "                if error_bars:\n",
    "                    ax.errorbar(\n",
    "                        datapoints[:, i],\n",
    "                        mus[:, i],\n",
    "                        yerr=stds[:, i],\n",
    "                        fmt=\"o\",\n",
    "                        markersize=4,\n",
    "                        markeredgecolor=cmap(norm_n(num_parameters[0, i])),\n",
    "                        markerfacecolor=cmap(norm_n(num_parameters[0, i])),\n",
    "                        ecolor=cmap(norm_n(num_parameters[0, i])),\n",
    "                        capsize=4,\n",
    "                        linestyle=\"none\",\n",
    "                    ) \n",
    "                else:\n",
    "                    ax.plot(datapoints[:, i], mus[:, i], \"o\", ms=4, color=cmap(norm_n(num_parameters[0, i])))\n",
    "                data_col = datapoints[:, i]\n",
    "                dmin = np.min(data_col)\n",
    "                dmax = np.max(data_col)\n",
    "                xfit = np.logspace(np.log10(2000), np.log10(200000), 200)\n",
    "                yfit = func((xfit, np.full_like(xfit, num_parameters[0, i])), *popt)\n",
    "                ax.plot(xfit, yfit, \"--\", color=cmap(norm_n(num_parameters[0, i])), zorder=-1)\n",
    "            ax.set_xlabel(r\"$D$\")\n",
    "            ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "            yticks = [0.50, 0.75, 1.00, 1.25, 1.50, 1.75]\n",
    "            ax.set_xlim(2000, 200000)\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xticks(np.unique(datapoints))\n",
    "            ax.xaxis.set_major_locator(FixedLocator(np.unique(datapoints)))\n",
    "            ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "            ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(int(x))))\n",
    "            ax.yaxis.set_minor_locator(FixedLocator(yticks))\n",
    "            ax.yaxis.set_major_locator(FixedLocator(yticks))\n",
    "            ax.yaxis.set_minor_formatter(ScalarFormatter())\n",
    "            ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "            ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "            sm_np = plt.cm.ScalarMappable(norm=norm_n, cmap=cmap)\n",
    "            sm_np.set_array([])\n",
    "            cbar_top1 = fig.colorbar(sm_np, ax=ax, pad=0.02, label=r\"$N$\")\n",
    "            cbar_top1.set_ticks(num_parameters[0, :])\n",
    "            cbar_top1.ax.set_ylim(np.min(num_parameters[0, :]), np.max(num_parameters[0, :]))\n",
    "            cbar_top1.ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(x)))\n",
    "            xlim = ax.get_xlim()\n",
    "            if error_bars:\n",
    "                ax.errorbar(\n",
    "                    [1, 2],\n",
    "                    [101, 102],\n",
    "                    yerr=[0.1, 0.2],\n",
    "                    fmt=\"o\",\n",
    "                    markersize=4,\n",
    "                    markeredgecolor=\"k\",\n",
    "                    markerfacecolor=\"k\",\n",
    "                    ecolor=\"k\",\n",
    "                    capsize=4,\n",
    "                    linestyle=\"none\",\n",
    "                    label=\"Data\",\n",
    "                ) \n",
    "            else:\n",
    "                ax.plot([1, 2], [101, 102], \"o\", ms=4, color=\"k\", label=\"Data\")\n",
    "            ax.plot([1, 2], [101, 102], \"--\", color=\"k\", label=\"Fit\")\n",
    "            ax.legend(handlelength=1.25, loc=\"lower left\")\n",
    "            ax.set_xlim(xlim)\n",
    "            ax.set_ylim(lmin, lmax)\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            order = [labels.index(\"Data\"), labels.index(\"Fit\")] # desired ordering\n",
    "            ax.legend(\n",
    "                [handles[i] for i in order], [labels[i] for i in order], \n",
    "                handlelength=1.25, \n",
    "                loc=\"lower left\",\n",
    "            )\n",
    "            \n",
    "            # plot the valdation loss over N for every D\n",
    "            ax = fig.add_subplot(gs[0, 0])  \n",
    "            norm_d = LogNorm(vmin=np.min(datapoints), vmax=np.max(datapoints))\n",
    "            for i in range(num_parameters.shape[0]):\n",
    "                if error_bars:\n",
    "                    ax.errorbar(\n",
    "                        num_parameters[i, :],\n",
    "                        mus[i, :],\n",
    "                        yerr=stds[i, :],\n",
    "                        fmt=\"o\",\n",
    "                        markersize=4,\n",
    "                        markeredgecolor=cmap(norm_d(datapoints[i, 0])),\n",
    "                        markerfacecolor=cmap(norm_d(datapoints[i, 0])),\n",
    "                        ecolor=cmap(norm_d(datapoints[i, 0])),\n",
    "                        capsize=4,\n",
    "                        linestyle=\"none\",\n",
    "                        label=fmt_param(datapoints[i, 0]),\n",
    "                    ) \n",
    "                else:\n",
    "                    ax.plot(\n",
    "                        num_parameters[i, :], \n",
    "                        mus[i, :], \n",
    "                        \"o\", \n",
    "                        label=fmt_param(datapoints[i, 0]), \n",
    "                        color=cmap(norm_d(datapoints[i, 0])),\n",
    "                    )\n",
    "                nmin = np.nanmin(num_parameters[i, :])\n",
    "                nmax = np.nanmax(num_parameters[i, :])\n",
    "                xfit = np.logspace(np.log10(2e5), np.log10(2e8), 200)\n",
    "                yfit = func((np.full_like(xfit, datapoints[i, 0]), xfit), *popt)\n",
    "                ax.plot(xfit, yfit, \"--\", color=cmap(norm_d(datapoints[i, 0])), zorder=-1)\n",
    "            ax.set_xlabel(r\"$N$\")\n",
    "            ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "            ax.set_title(model_label_helper(model), pad=3)\n",
    "            yticks = ax.get_yticks()\n",
    "            if model == \"2b\":\n",
    "                ax.set_xlim(2e5, 1e8)\n",
    "            else:\n",
    "                ax.set_xlim(2e5, 2e8)\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(int(x))))\n",
    "            ax.set_xlim(200000, 200000000)\n",
    "            ax.yaxis.set_minor_locator(FixedLocator(yticks))\n",
    "            ax.yaxis.set_major_locator(FixedLocator(yticks))\n",
    "            ax.yaxis.set_minor_formatter(ScalarFormatter())\n",
    "            ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "            ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "            ax.set_ylim(lmin, lmax)\n",
    "            sm_d = plt.cm.ScalarMappable(norm=norm_d, cmap=cmap)\n",
    "            sm_d.set_array([])\n",
    "            cbar_top2 = fig.colorbar(sm_d, ax=ax, pad=0.02, label=r\"$D$\")\n",
    "            cbar_top2.set_ticks(datapoints[:, 0])\n",
    "            cbar_top2.ax.set_ylim(np.min(datapoints[:, 0]), np.max(datapoints[:, 0]))\n",
    "            cbar_top2.ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(x)))\n",
    "        \n",
    "            # plot the full L(D,N)\n",
    "            ax = fig.add_subplot(gs[2, 0]) \n",
    "            x_centers = datapoints[:, 0]\n",
    "            y_centers = num_parameters[0, :]\n",
    "            def _make_edges(centers: np.ndarray) -> np.ndarray:\n",
    "                \"\"\"\n",
    "                Small helper function to find the edges of bins in log space.\n",
    "                \"\"\"\n",
    "                logc   = np.log10(centers)\n",
    "                strides = np.diff(logc) * 0.5\n",
    "                edges  = np.concatenate(\n",
    "                    [\n",
    "                        [logc[0] - strides[0]], \n",
    "                        logc[:-1] + strides, \n",
    "                        [logc[-1] + strides[-1]]\n",
    "                    ]\n",
    "                )\n",
    "                return 10**edges\n",
    "            d_edges = _make_edges(x_centers)\n",
    "            n_edges = _make_edges(y_centers)\n",
    "            cmin = 0.45\n",
    "            cmax = 1.77\n",
    "            cstep = 0.33\n",
    "            pcm = ax.pcolormesh(d_edges, n_edges, mus, cmap=cmap_name.rstrip(\"_r\"), vmin=cmin, vmax=cmax, shading=\"auto\")\n",
    "            pcm.set_rasterized(True)\n",
    "            ax.set_xlabel(r\"$D$\")\n",
    "            ax.set_ylabel(r\"$N$\")\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xlim(d_edges[0], d_edges[-1])\n",
    "            ax.set_ylim(n_edges[0], n_edges[-1])\n",
    "            ax.set_xticks(np.unique(datapoints))\n",
    "            ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(int(x))))\n",
    "            ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: fmt_param(int(x))))\n",
    "            ax.tick_params(axis=\"x\", which=\"minor\", length=0)\n",
    "            cbar = fig.colorbar(pcm, ax=ax, pad=0.02, label=r\"$L_\\mathrm{val}$\") \n",
    "            cbar.ax.set_ylim([cmin, cmax])\n",
    "            cbar.locator = FixedLocator(np.arange(cmin, cmax + cstep, cstep))\n",
    "            cbar.formatter = ScalarFormatter() \n",
    "            cbar.update_ticks()\n",
    "            \n",
    "            # align labels and save the figure\n",
    "            fig.align_labels()\n",
    "            fig.savefig(os.path.join(fig_dir, f\"{model:s}_{fit_type:s}_{metric_key:s}_scaling_map_v2.pdf\"))\n",
    "            fig.savefig(os.path.join(fig_dir, f\"{model:s}_{fit_type:s}_{metric_key:s}_scaling_map_v2.svg\"))\n",
    "            \n",
    "        # build and export the latex table for the fit parameters & aicc\n",
    "        if param_rows:\n",
    "            df = pd.DataFrame(param_rows)\n",
    "            latex_params_global = df.to_latex(\n",
    "                index=True,\n",
    "                escape=False,\n",
    "                longtable=False,\n",
    "                multicolumn=True,\n",
    "                multicolumn_format=\"c\",\n",
    "                bold_rows=False,\n",
    "                column_format=len(df.columns) * r\"c@{\\hspace{1em}}\",\n",
    "            )\n",
    "            print(f\"Fit parameters: {fit_type:s} & {metric_key:s}\")\n",
    "            print(latex_params_global)\n",
    "            with open(os.path.join(fig_dir, f\"{fit_type:s}_{metric_key:s}_scaling_fits.txt\"), \"w\") as f:\n",
    "                f.write(latex_params_global)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
