{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer hyperparameter scaling\n",
    "\n",
    "This notebook has been written to run on the TU Ilmenau cluster with my specific setup.\n",
    "However, it can also be used simply to visualize the learning rate scaling laws. \n",
    "\n",
    "A summary of all results is also stored in the directory `/research/scaling_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import ticker  \n",
    "from matplotlib.ticker import FixedLocator\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "import optimetal.utils as utils\n",
    "import optimetal.factory as factory\n",
    "from optimetal.data.loader import load_torch_data\n",
    "utils.load_plot_style()\n",
    "\n",
    "def load_tb_scalars(logdir: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load scalar values from tensorboard event files. This is useful\n",
    "    when you want to look at training and validation loss curves.\n",
    "    \"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        logdir,\n",
    "        size_guidance={event_accumulator.SCALARS: 0},\n",
    "    )\n",
    "    ea.Reload()\n",
    "    tags = ea.Tags().get(\"scalars\", [])\n",
    "    tb_log = {}\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        values = [e.value for e in events]\n",
    "        tb_log[tag] = values\n",
    "    return tb_log\n",
    "\n",
    "def load_results(study_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load all results from the transformer hyperparameter scaling law study.\n",
    "    Input:\n",
    "        study_path:     Path to the root directory containing subdirectories from model training\n",
    "    Output:\n",
    "        scaling_laws:   Nested dict mapping, model type, scaling type, and hyperparameters to dictionaries with \"val_loss\"\n",
    "    \"\"\"\n",
    "\n",
    "    # gather all study directories\n",
    "    study_dirs = [d for d in os.listdir(study_path) if os.path.isdir(os.path.join(study_path, d))]\n",
    "    # initialize nested structure for results\n",
    "    results = {}\n",
    "    # iterate through each study directory and load results\n",
    "    print(f\"Loading scaling law results for {len(study_dirs):d} models\")\n",
    "    for study_dir in study_dirs:\n",
    "        # path setup and checks\n",
    "        study_dir_path = os.path.join(study_path, study_dir)\n",
    "        val_loss_path = os.path.join(study_dir_path, \"val_loss.txt\")\n",
    "        best_model_path = os.path.join(study_dir_path, \"best_model.pt\")\n",
    "        if not os.path.exists(val_loss_path) or not os.path.exists(best_model_path):\n",
    "            print(f\"Skipping {study_dir_path:s}, probably still running\")\n",
    "            continue\n",
    "        # get the number of model parameters\n",
    "        best_model_dict = load_torch_data(best_model_path)\n",
    "        config_dict = best_model_dict[\"config_dict\"]\n",
    "        model_config = config_dict.architecture\n",
    "        model = factory.create_model(model_config)\n",
    "        num_parameter = utils.get_model_parameters(model)\n",
    "        # parse metadata\n",
    "        width = re.search(r\"hidden(\\d+)\", study_dir).group(1)\n",
    "        gamma = re.search(r\"gamma(\\d+\\.?\\d*)\", study_dir).group(1)\n",
    "        seed = re.search(r\"seed(\\d+)\", study_dir).group(1)\n",
    "        # load the data from the tensorboard log and validation loss file\n",
    "        best_val_loss = float(np.loadtxt(val_loss_path))\n",
    "        tb_log = load_tb_scalars(study_dir_path)\n",
    "        val_loss = tb_log.get(\"val/loss\", [])\n",
    "        min_idx = np.argmin(val_loss)\n",
    "        best_eps_loss = tb_log.get(\"val/eps\", [])[min_idx]\n",
    "        best_drude_loss = tb_log.get(\"val/drude\", [])[min_idx]\n",
    "        result_entry = {\n",
    "            \"seed\": seed,\n",
    "            \"num_parameter\": num_parameter,\n",
    "            \"lr\": config_dict.optimizer[\"lr\"],\n",
    "            \"val_loss\": best_val_loss,\n",
    "            \"eps_loss\": best_eps_loss,\n",
    "            \"drude_loss\": best_drude_loss,\n",
    "        }\n",
    "        # insert the data into the nested structure\n",
    "        results.setdefault(gamma, {}).setdefault(width, []).append(result_entry)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directory containing the scaling law study\n",
    "study_path = \"/scratch/magr4985/Transformer_Scaling\"\n",
    "\n",
    "# directory to save the results\n",
    "output_dir = \"./scaling_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# check if the study path exists, else just load in the results already stored in JSON files\n",
    "json_path = os.path.join(output_dir, \"transformer_scaling_results.json\")\n",
    "if os.path.exists(study_path) and not os.path.exists(json_path):\n",
    "    print(f\"Study path {study_path:s} exists, loading results from there\")\n",
    "    results = load_results(study_path)\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "else:\n",
    "    print(f\"Loading results from JSON file\")\n",
    "    with open(json_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "# figure directory\n",
    "fig_dir = \"./scaling_data/lr_scaling\"\n",
    "os.makedirs(fig_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the data, i.e., how the validation loss behaves when training models\n",
    "with different widths using different learning rate scaling exponents.\n",
    "\"\"\"\n",
    "\n",
    "ms = 4 # marker size\n",
    "\n",
    "# figure setup\n",
    "fig = plt.figure(figsize=(3.5, 5))\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1, 0.05], height_ratios=[1, 1])\n",
    "\n",
    "# plot scaling law, i.e., validation loss over width, for different learning rate scaling exponents\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "norm = Normalize(vmin=0, vmax=2)\n",
    "cmap = plt.get_cmap(\"viridis\") \n",
    "for gamma in results:\n",
    "    color = cmap(norm(float(gamma))) \n",
    "    width_dict = results[gamma]\n",
    "    widths = sorted([int(key) for key in width_dict])\n",
    "    model_parameter = []\n",
    "    mean_val_loss = []\n",
    "    for w in widths:\n",
    "        model_parameter.append(width_dict[str(w)][0][\"num_parameter\"])\n",
    "        mean = np.mean(np.array([d[\"val_loss\"] for d in width_dict[str(w)]], dtype=float))\n",
    "        mean_val_loss.append(mean)\n",
    "    ax.plot(widths, mean_val_loss, \"o-\", markersize=ms, color=color)\n",
    "    \n",
    "# axis labels ticks\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xticks(widths)\n",
    "ax.set_xticklabels(widths)\n",
    "ax.xaxis.set_major_locator(FixedLocator(widths))\n",
    "ax.tick_params(axis=\"x\", which=\"minor\", length=0)\n",
    "ax.set_xlabel(r\"$d_\\mathrm{h}$\")\n",
    "ax.set_ylim([1.0, 1.4])\n",
    "ax.set_yticks([1.0, 1.1, 1.2, 1.3, 1.4])\n",
    "ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "\n",
    "# colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "cbar_ax = fig.add_subplot(gs[0, 1])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax, label=r\"$\\gamma$\", aspect=25)\n",
    "cbar.ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "cbar.ax.set_yticks([0.0, 0.5, 1.0, 1.5, 2.0])\n",
    "\n",
    "# plot validation loss over learning rate scaling exponents for different model widths\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "norm = LogNorm(vmin=16, vmax=1024)\n",
    "cmap = plt.get_cmap(\"viridis\") \n",
    "for width in widths:\n",
    "    color = cmap(norm(int(width))) \n",
    "    gamma_values = []\n",
    "    mean_val_loss = []\n",
    "    for gamma in results:\n",
    "        gamma_values.append(float(gamma))\n",
    "        width_dict = results[gamma][str(width)]\n",
    "        mean = np.mean(np.array([d[\"val_loss\"] for d in width_dict], dtype=float))\n",
    "        mean_val_loss.append(mean)\n",
    "    gamma_values = np.array(gamma_values)\n",
    "    mean_val_loss = np.array(mean_val_loss)\n",
    "    sort_idx = np.argsort(gamma_values)\n",
    "    gamma_values = gamma_values[sort_idx]\n",
    "    mean_val_loss = mean_val_loss[sort_idx]\n",
    "    ax.plot(gamma_values, mean_val_loss, \"o-\", markersize=ms, color=color)\n",
    "\n",
    "# axis labels ticks\n",
    "ax.set_xlabel(r\"$\\gamma$\")\n",
    "ax.set_ylim([1.0, 1.4])\n",
    "ax.set_yticks([1.0, 1.1, 1.2, 1.3, 1.4])\n",
    "ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "\n",
    "# colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "cbar_ax = fig.add_subplot(gs[1, 1])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax, label=r\"$d_\\mathrm{h}$\", aspect=25)\n",
    "cbar_ticks = widths \n",
    "cbar.ax.set_yticks(cbar_ticks)\n",
    "cbar.ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "cbar.ax.yaxis.set_major_formatter(ticker.FormatStrFormatter(\"%d\"))\n",
    "\n",
    "# adjust figure layout and save the figure\n",
    "fig.tight_layout()\n",
    "fig.align_labels()\n",
    "fig.savefig(os.path.join(fig_dir, \"transformer_lr_scaling_1.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the data, i.e., how the validation loss behaves when training models\n",
    "with different widths using different learning rate scaling exponents.\n",
    "\"\"\"\n",
    "\n",
    "ms = 4 # marker size\n",
    "\n",
    "# figure setup\n",
    "fig = plt.figure(figsize=(3.5, 3))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.05])\n",
    "\n",
    "# plot scaling law, i.e., validation loss over width, for different learning rate scaling exponents\n",
    "ax = fig.add_subplot(gs[0])\n",
    "norm = Normalize(vmin=0, vmax=2)\n",
    "cmap = plt.get_cmap(\"viridis\") \n",
    "for gamma in results:\n",
    "    color = cmap(norm(float(gamma))) \n",
    "    width_dict = results[gamma]\n",
    "    widths = sorted([int(key) for key in width_dict])\n",
    "    model_parameter = []\n",
    "    mean_val_loss = []\n",
    "    for w in widths:\n",
    "        model_parameter.append(width_dict[str(w)][0][\"num_parameter\"])\n",
    "        mean = np.mean(np.array([d[\"val_loss\"] for d in width_dict[str(w)]], dtype=float))\n",
    "        mean_val_loss.append(mean)\n",
    "    ax.plot(widths, mean_val_loss, \"o-\", markersize=ms, color=color)\n",
    "    \n",
    "# axis labels ticks\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xticks(widths)\n",
    "ax.set_xticklabels(widths)\n",
    "ax.xaxis.set_major_locator(FixedLocator(widths))\n",
    "ax.tick_params(axis=\"x\", which=\"minor\", length=0)\n",
    "ax.set_xlabel(r\"$d_\\mathrm{h}$\")\n",
    "ax.set_ylim([1.0, 1.4])\n",
    "ax.set_yticks([1.0, 1.1, 1.2, 1.3, 1.4])\n",
    "ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "\n",
    "# colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "cbar_ax = fig.add_subplot(gs[1])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax, label=r\"$\\gamma$\", aspect=25)\n",
    "cbar.ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "cbar.ax.set_yticks([0.0, 0.5, 1.0, 1.5, 2.0])\n",
    "\n",
    "# adjust figure layout and save the figure\n",
    "fig.tight_layout()\n",
    "fig.align_labels()\n",
    "fig.savefig(os.path.join(fig_dir, \"transformer_lr_scaling_1_v1.pdf\"))\n",
    "\n",
    "# figure setup\n",
    "fig = plt.figure(figsize=(3.5, 3))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.05])\n",
    "\n",
    "# plot validation loss over learning rate scaling exponents for different model widths\n",
    "ax = fig.add_subplot(gs[0])\n",
    "norm = LogNorm(vmin=16, vmax=1024)\n",
    "cmap = plt.get_cmap(\"viridis\") \n",
    "for width in widths:\n",
    "    color = cmap(norm(int(width))) \n",
    "    gamma_values = []\n",
    "    mean_val_loss = []\n",
    "    for gamma in results:\n",
    "        gamma_values.append(float(gamma))\n",
    "        width_dict = results[gamma][str(width)]\n",
    "        mean = np.mean(np.array([d[\"val_loss\"] for d in width_dict], dtype=float))\n",
    "        mean_val_loss.append(mean)\n",
    "    gamma_values = np.array(gamma_values)\n",
    "    mean_val_loss = np.array(mean_val_loss)\n",
    "    sort_idx = np.argsort(gamma_values)\n",
    "    gamma_values = gamma_values[sort_idx]\n",
    "    mean_val_loss = mean_val_loss[sort_idx]\n",
    "    ax.plot(gamma_values, mean_val_loss, \"o-\", markersize=ms, color=color)\n",
    "\n",
    "# axis labels ticks\n",
    "ax.set_xlabel(r\"$\\gamma$\")\n",
    "ax.set_ylim([1.0, 1.4])\n",
    "ax.set_yticks([1.0, 1.1, 1.2, 1.3, 1.4])\n",
    "ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "\n",
    "# colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "cbar_ax = fig.add_subplot(gs[1])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax, label=r\"$d_\\mathrm{h}$\", aspect=25)\n",
    "cbar_ticks = widths \n",
    "cbar.ax.set_yticks(cbar_ticks)\n",
    "cbar.ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "cbar.ax.yaxis.set_major_formatter(ticker.FormatStrFormatter(\"%d\"))\n",
    "\n",
    "# adjust figure layout and save the figure\n",
    "fig.tight_layout()\n",
    "fig.align_labels()\n",
    "fig.savefig(os.path.join(fig_dir, \"transformer_lr_scaling_1_v2.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyze which learning rate works best for each model width and how it scales with the number of model parameters.\n",
    "\"\"\"\n",
    "\n",
    "ms = 4 # marker size\n",
    "\n",
    "# figure setup\n",
    "fig = plt.figure(figsize=(3.5, 5))\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1, 0.05], height_ratios=[1, 1])\n",
    "\n",
    "# validation loss vs learning rate for each model width\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "norm = LogNorm(vmin=16, vmax=1024)\n",
    "cmap = plt.get_cmap(\"viridis\") \n",
    "model_parameter = []\n",
    "best_lr_per_width = []\n",
    "best_val_loss_per_width = []\n",
    "for width in widths:\n",
    "    color = cmap(norm(int(width))) \n",
    "    lr_list = []\n",
    "    mean_val_loss = []\n",
    "    for gamma in results:\n",
    "        data = results[gamma][str(width)]\n",
    "        lr_list.append(data[0][\"lr\"])\n",
    "        mean = np.mean(np.array([d[\"val_loss\"] for d in data], dtype=float))\n",
    "        mean_val_loss.append(mean)\n",
    "    model_parameter.append(data[0][\"num_parameter\"])\n",
    "    lr_list = np.array(lr_list)\n",
    "    mean_val_loss = np.array(mean_val_loss)\n",
    "    min_idx = np.argmin(mean_val_loss)\n",
    "    best_lr_per_width.append(lr_list[min_idx])\n",
    "    best_val_loss_per_width.append(mean_val_loss[min_idx])\n",
    "    ax.plot(lr_list, mean_val_loss, \"o-\", markersize=ms, color=color)\n",
    "ax.plot(best_lr_per_width, best_val_loss_per_width, \"k-\")\n",
    "\n",
    "# axis labels ticks\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(r\"$\\eta_\\mathrm{max}$\")\n",
    "ax.set_ylabel(r\"$L_\\mathrm{val}$\")\n",
    "\n",
    "# colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "cbar_ax = fig.add_subplot(gs[0, 1])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax, label=r\"$d_\\mathrm{h}$\", aspect=25)\n",
    "cbar_ticks = widths \n",
    "cbar.ax.set_yticks(cbar_ticks)\n",
    "cbar.ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "cbar.ax.yaxis.set_major_formatter(ticker.FormatStrFormatter(\"%d\"))\n",
    "\n",
    "# fit the best learning rate depending on the number of model parameters\n",
    "base_model_parameter = model_parameter[4]\n",
    "log_x = np.log(np.array(model_parameter) / base_model_parameter) # use the 'width = 256' as reference point\n",
    "log_y = np.log(best_lr_per_width)\n",
    "slope, intercept = np.polyfit(log_x, log_y, 1) # unweighted linear fit\n",
    "gamma = -slope # slope = -alpha\n",
    "eta0 = np.exp(intercept)\n",
    "x_fit = np.logspace(np.log10(min(model_parameter)), np.log10(max(model_parameter)), 100)\n",
    "y_fit = eta0 / ((x_fit / base_model_parameter) ** gamma)\n",
    "\n",
    "# plot the empirical learning rate scaling law\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax.plot(model_parameter, best_lr_per_width, \"ko-\", markersize=ms)\n",
    "ax.plot(x_fit, y_fit, \"k--\")\n",
    "ax.annotate(\n",
    "    f\"$\\\\eta_\\\\mathrm{{max}}(N) = \\\\eta^\\\\mathrm{{c}}_\\\\mathrm{{max}} \\\\cdot \\\\left(N_0 / N\\\\right)^{{{gamma:.2f}}}$\",\n",
    "    xy=(x_fit[50], y_fit[50]),\n",
    "    xytext=(-20, 30),\n",
    "    textcoords=\"offset points\",\n",
    "    color=\"k\",\n",
    "    fontsize=8,\n",
    "    arrowprops=dict(arrowstyle=\"->\", lw=1, color=\"k\"),\n",
    "    ha=\"left\", va=\"bottom\",\n",
    ")\n",
    "\n",
    "# axis labels ticks\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(r\"$N$\")\n",
    "ax.tick_params(axis=\"x\", which=\"minor\", length=0)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis=\"y\", which=\"minor\", length=0)\n",
    "ax.set_ylabel(r\"Optimal $\\eta_\\mathrm{max}(N)$\")\n",
    "\n",
    "# global figure settings\n",
    "fig.tight_layout()\n",
    "fig.align_labels()\n",
    "fig.savefig(os.path.join(fig_dir, \"transformer_lr_scaling_2.pdf\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
