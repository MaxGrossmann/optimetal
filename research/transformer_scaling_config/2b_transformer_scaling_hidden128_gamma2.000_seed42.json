{
    "seed": 42,
    "trial_dir": "/scratch/magr4985/Transformer_Scaling/2b_transformer_scaling_hidden128_gamma2.000_seed42",
    "num_train_data": 20000,
    "batch_size": 256,
    "architecture": {
        "activation": "relu",
        "depth": 2,
        "edge_embedding_dict": {
            "apply_envelope": true,
            "basis_width": 4.0,
            "num_basis": 32,
            "type": "gaussian"
        },
        "hidden_dim": 128,
        "message_passing_dict": {
            "heads": 2,
            "hidden_multiplier": 6,
            "num_layers": 2,
            "type": "transformer"
        },
        "node_embedding_dict": {
            "type": "group_period"
        },
        "pooling_dict": {
            "type": "vector_attention"
        },
        "spectra_dim": 512,
        "twobody_cutoff": 5.5,
        "type": "optimetal_2b"
    },
    "optimizer": {
        "type": "adamw",
        "lr": 0.004211023003214593,
        "weight_decay": 1e-06
    },
    "loss_fn_eps": "mae",
    "loss_fn_drude": "mae",
    "lr_scheduler": {
        "type": "cosineannealing",
        "T_max": 500,
        "eta_min": 0.0
    },
    "warmup_epochs": 5,
    "grad_clip": 100,
    "eps_weight": 1,
    "drude_weight": 1,
    "early_stopping": false,
    "patience": 500,
    "num_epoch": 500,
    "precision": "bf16"
}