{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4d39abe9",
            "metadata": {},
            "source": [
                "### Evaluation notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36f485ee",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "Run 'train_model.py' or 'train_model.ipynb' first...\n",
                "\"\"\"\n",
                "\n",
                "import os\n",
                "import sys\n",
                "from importlib.resources import files\n",
                "\n",
                "from optimetal.evaluation import Evaluator\n",
                "from optimetal.data.loader import load_torch_data, create_dataloader\n",
                "\n",
                "\"\"\"\n",
                "-----------------------------------------------------------------------------------------------------------------------\n",
                "START OF USER INPUT:\n",
                "\"\"\"\n",
                "\n",
                "# path to a 'best_model.pt' file created by 'optimetal.training.Trainer'\n",
                "model_name = \"optimetal3b_seed42\"\n",
                "best_model_path = files(\"optimetal.files\").joinpath(f\"{model_name:s}.pt\")\n",
                "\n",
                "# path to the dataset we want to evaluate\n",
                "eval_path = \"../graph/test.pt\" # 'val.pt' or 'test.pt'\n",
                "\n",
                "# batch size (speeds up the process at bit)\n",
                "batch_size = 32 # adjust this according to your GPU memory\n",
                "\n",
                "# device index (0 for GPU 0, 1 for GPU 1, etc. or -1 for CPU)\n",
                "device_index = 0\n",
                "\n",
                "# plot directory\n",
                "eval_result_dir = f\"./best_model_eval/{model_name:s}\"\n",
                "\n",
                "\"\"\"\n",
                "END OF USER INPUT:\n",
                "-----------------------------------------------------------------------------------------------------------------------\n",
                "\"\"\"\n",
                "\n",
                "# sanity checks\n",
                "if not os.path.exists(eval_path):\n",
                "    sys.exit(\"The path 'eval_path' does not exist (evaluation data not found)\")\n",
                "if batch_size < 2:\n",
                "    raise ValueError(\"The 'batch_size' must be larger than 1\")\n",
                "\n",
                "# output directory setup\n",
                "os.makedirs(eval_result_dir, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "578f1483",
            "metadata": {},
            "outputs": [],
            "source": [
                "# load the data on which we want to evaluate the model\n",
                "eval_data = load_torch_data(eval_path)\n",
                "dataloader = create_dataloader(\n",
                "    eval_data, \n",
                "    num_data=-1, # use the whole dataset \n",
                "    batch_size=batch_size,\n",
                "    shuffle=False, # do not shuffle\n",
                ")\n",
                "print(f\"Loaded evaluation data from '{eval_path:s}'\", flush=True)\n",
                "    \n",
                "# setup the evaluator\n",
                "evaluator = Evaluator(\n",
                "    best_model_path=best_model_path, \n",
                "    dataloader=dataloader,\n",
                "    device_index=device_index,\n",
                "    dataset_name=eval_path.split(\"/\")[-1].split(\".\")[0]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d7d8bd6a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# evaluate the model\n",
                "evaluator.evaluate()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0458380c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# print and store metrics\n",
                "# (all results/plots/files will found in the 'eval_result_dir' directory)\n",
                "evaluator.print_metrics()\n",
                "evaluator.store_metrics(metric_path=os.path.join(eval_result_dir, \"metrics.json\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2dab7f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# make a nice quantile plots\n",
                "fig, gs = evaluator.quantile_plot(fig_path=os.path.join(eval_result_dir, \"quantiles.pdf\"), rng_seed=42)\n",
                "fig, gs = evaluator.quantile_plot(interband_only=True, fig_path=os.path.join(eval_result_dir, \"quantiles_interband.pdf\"), rng_seed=42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4dc9ba75",
            "metadata": {},
            "outputs": [],
            "source": [
                "#  plots dielectric function of some random materials\n",
                "evaluator.store_metrics(metric_path=os.path.join(eval_result_dir, \"metrics.json\"))\n",
                "fig, ax = evaluator.plot_rand_mats(\n",
                "    num_mats=16, \n",
                "    fig_path=os.path.join(eval_result_dir, \"rand_mats.pdf\"),\n",
                "    rng_seed=42,\n",
                ")\n",
                "fig, ax = evaluator.plot_rand_mats(\n",
                "    num_mats=16, \n",
                "    interband_only=True,\n",
                "    fig_path=os.path.join(eval_result_dir, \"rand_mats_interband.pdf\"),\n",
                "    rng_seed=42,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "95bb65fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Available metrics for histograms and violin plots\")\n",
                "for key in evaluator.label_dict:\n",
                "    print(f'\"{key:s}\"')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9083969d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# analyze the distribution of a metrics (just an example...)\n",
                "metric = \"loss\"\n",
                "fig, ax = evaluator.metric_histogram(metric)\n",
                "fig, ax = evaluator.metric_violinplot(metric)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "914335cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# print the metric table in a latex format, ready to copy\n",
                "latex_table_str = evaluator.print_metric_latex_table()\n",
                "with open(os.path.join(eval_result_dir, \"latex_table.txt\"), \"w\") as f:\n",
                "    f.write(latex_table_str)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
